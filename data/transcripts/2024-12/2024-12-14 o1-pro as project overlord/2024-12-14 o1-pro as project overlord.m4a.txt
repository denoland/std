Tom
0:00:00
Okay, so if we can figure out how to get O1 to tell us what is important to do, what are the things we should bet the company on and which things we shouldn't, not just a one-off but an ongoing thing, then that seems of value to all enterprises. Being able to have a standard way to reason through all the things you think are important and then tell you what's actually important and what you should be pursuing is valuable but it also actually requires a social network or coordination because if you had say a thousand teams that were all using O1 to guide them they might come up with the right answer

Tom
0:00:44
in isolation but they'd still collide because they didn't talk. Right. And you'd get not only collisions, but you'd get, I'm not sure what it is, generations where you combine two ideas and a third is obvious to prool. Or an AI.

Tom
0:01:04
I think also if everyone's using O1 Pro as the project manager or as the the center of the North Star the center of the headmark If anyone's using that the chance of a collision goes way up because first of all I suspect O1 thinks in in buckets like if you ask a thousand different questions you'll end up with ten consistent buckets and so

Tom
0:01:34
And so... Spookinization is definitely better, isn't it? Yeah, so you'll end up sort of smacking into other people more likely than you would have if you'd just used your own human chaos to steer. But second of all, your collisions will also be higher because if you've logically reduced all of the other possibilities down, now your choices are literally like half a dozen and you all have that same half a

Tom
0:02:04
dozen so that's another form of bucketing or comb filter that's gonna make everyone collide and so that sort of seems to point at having the ability to socially reason so that we don't smack into each other and to be able to use O1 as the thing that tells us what we're doing.

3
0:02:26
No one's talking about that,

Tom
0:02:27
but it seems like if you could do that. And it informs, you know, randomly, there's this other team doing something very similar. Yeah. Yeah, but also being able to recruit, because, you know, people are looking for a team to join,

Tom
0:02:41
and they're like, man, everything's changing so fast, I don't know what to do. And then O1 helps them find their place. Yeah, but this is a bit of a side point, but teams are also dynamic. Teams are based on people turning up. So it's not an employee we're talking about. Employees actually are bad. So what are we going to do then? What are we going to do? Well it's dangerous to get excited and be like

Tom
0:03:18
cool toy cool toy like because I mean I want pro is the coolest toy that I've ever ever got my hands on like it is the coolest thing by none but that's not enough like okay now what? Everyone at 200 bucks a month has got the same tool. Yeah and it also it also points at what's coming. I mean, you know GPT-4 versus O1 Pro one year's difference Mate, yeah, like yeah, exactly. Yeah. Okay. So what do we do? What do we care about? What's valuable?

Tom
0:03:50
I think what's valuable is Okay, I'm just gonna throw some stuff on the table because the truth of answer is I don't know. Okay. Now, what do we have? We have consistently been ahead of the next drop by a number of days, weeks, months, whatever. And so what does that mean?

Tom
0:04:21
That means we're thinking in the same way as these guys with their billions who are

Scott
0:04:25
just churning this stuff out.

Tom
0:04:27
No, that's wrong. The objective answer to that is that how AI needs to land is deduction. And all that we're doing is we're deducing how it's going and we... I disagree or I may have misspoken. I suppose it doesn't matter to have a third round of argument about it you were really just trying to lean on this to build up to something. What I was trying to say is we're probably on the the soft wave right now.

Tom
0:05:07
We... I mean, that's rude, because we don't know that, but

Scott
0:05:13
what's your point with that, I guess?

Tom
0:05:15
No, well, I started off by saying, I don't know. So I'm just putting stuff on the table. Okay, so you're saying what have we got? Well, we're definitely immersed in the AI world, so there's nothing you know, like I speak to people

Tom
0:05:27
these days, they don't even know that O1's arrived, let alone O1 Pro, so I'm like, well, sure, but that's just awareness. Number two we seem to have a load that we can put on it that I'm not sure everyone has yet. I think if everyone worked really hard they could come up with a load like that. We are also in the very privileged position that we can actually

Tom
0:05:57
that's sort of rare in itself. We have a lot of experience with the feeling of what LLMs are like. We understand intuitively what's happening inside of them, which is sort of like knowing how to program. But even programming wasn't just reading a book. You got to do it a lot and then you get a sense for what's good code, bad code, all that. We have that sense with LLMs now.

Tom
0:06:33
Yeah. Yeah, so we can sense that we're conversant with the system. That's valuable. We know that there's a massive gaping hole for NAPs. It's starting to be sort of touched on by MCP We know that blockchains don't integrate with the software world Correctly or nicely we know that tokens poison blockchains But we didn't really know that before we just suspected it What else what we doing

Tom
0:07:02
What we're doing all right, and is it is it a valid concern to say, okay, I've seen a one pro in action. I recognize when it's smarter than me. It is not just a little bit smarter. It's like, uh, it's every time smarter and, um, whatever it says makes logical sense. Like it no longer, it's, it's like it has an internal logic model. I've not once made it say sorry. Like I've never

Tom
0:07:41
ever called it out. I've never called it out. I've used this thing all day every day since it landed. All day every day. Morning, night, thinking about things to put in, refining things, regenerating things, all that. Not once have I been like, hey mate you said this and then you said that. What gives? Which is a common occurrence with 4.0, not a single thing. I've been taking code that it runs, code that it wrote, like two pages worth at a time, and it runs, start to finish, it runs. Like 4.0 was not that. So because of that, I overzealously was sort of paying homage or respect to the machine,

Tom
0:08:26
but the consequence is that I no longer trust my own decision-making ability to decide what's best for my life, for one, but second, for the Dreamcatcher and for the Artifact blockchain because I want to hear it from O1. I don't want to hear it from you, me or anyone else. I want to be able to follow O1. Like I want it to tell me what of all the things that I've chucked at it and all the questions and prods and doubts that I've given it.

Tom
0:09:08
I want it to decide or reason with me or tell me what's the best thing? Because if I'm following that, I trust that O1 can defend my actions against any onslaught. Anybody, anywhere, any human could argue with O1 and be like, Tom made a bad decision here. And it would defend me. It'd be like, nah mate, ABCDY, therefore, blah blah blah, go stuff yourself. The only thing that's going to correct O1 is O2. Maybe, right? And even that's a maybe because the smartest guy, smartest entity on the planet, there's only so many moves on the chessboard.

Tom
0:09:49
And provided you understood the rules right, you know, once you can, there is like, if we assume that there's a perfect move, what's even God themselves going to make, right? It's like that move. So you don't need anything more than that. So I think for the time being, and I'm less and less certain this is going to last, but for the time being, O1 gives options. And number one, being able to talk to the O1 pro is a skill.

Tom
0:10:24
You're right. You need to be conversant about how to actually use this. Yeah, and you need to understand what it does and doesn't know and all that, but it's much better at, it's much better calibrated. It knows its own limitations and it knows when it's sure about something. This is when, back in June, we were talking about yeah and they decided to roll into this marketing 12-day thing so I love it I love it like these guys are so great. I don't. I mean you gotta. Every fucking day there's another thing to. It's great it's great and but you got to also think like

Tom
0:11:06
internally surely they are sitting there with O1 Pro under API mode, asking it what they should do, surely?

Scott
0:11:19
Well, which probably is why they've got... I've never heard of any company do 12 drops day after day, and each drop being something substantive.

Tom
0:11:31
Substantive, right? Yeah, it's... this is, you know... And day one, day one was like the greatest intelligence on earth for like the highest price consumer subscription I ever Yeah, and look at Sora We cannot buy GPUs fast enough. Yeah in order to meet the demand on Sora I still have five pounds good

Tom
0:11:54
Scots pounds on the table should you wish to take the bet that the 12th day is going to be the API drop but we'll see. I'll take that bet. I'll bet it's not. Yeah, I'll bet day 12 is not the API.

Scott
0:12:11
Well, how about we wait on it?

Tom
0:12:14
The API is part of the 12th? I'll bet you, yeah I'll bet you £5 that it's not part of the 12th. Alright, okay. So, the point being is, there is, I think we're sitting on a pile of gold which we don't actually use. Right, but the thing that stops us from spending it, because we've got time, we've got resources in the bank, we've got experience, we're like schooled up hard on this stuff, we've got

Tom
0:13:06
software in the pipe, we've got capabilities, and we've got like just opportunity trying to break down the door. And we're sitting here being like, um, what's the right move? Because there are wrong moves here and wrong moves are to, um, make a move that, like, I don't know if you've ever been hydrofoiling, but it feels like a very delicate balance where we get to ride the wave so long as we don't dip too far.

Scott
0:13:36
I have.

Tom
0:13:37
And you can... I got to the white towel. Yeah, yeah, yeah. And so any activity that we take that isn't like cumulatively building to accelerate us further is a loss. It feels... I don't know, to be honest, I'm getting more rocked and anxious than it was? I'm about as anxious as I've ever been right now. This is the most anxious because for me, the bottom just fell out of software.

Tom
0:14:12
Like, we talked about it and we said it was coming and we even laughed about it because we were like, we know how to make money off it. Last November, it was all beautiful. And then it arrived, it actually arrived and I was able to publish software modules

Tom
0:14:24
from half an hour's worth of prompting and I'm like oh that's...

Scott
0:14:30
And it's only going to get better with context. You can just ask for something to happen.

Tom
0:14:34
Right and then at some point the human detaches and you're like what is the human doing anymore? But then we hit this other realm which is like well reasoning. Reasoning is hard. Okay, so here's my best shot at it and I'm getting more and more queasy about the Basilisk. The best shot is arbitrage between people who haven't spent 14 hours a day using this thing. Right, yeah, but you could argue that that's the only thing that's ever been saleable really is information gradient. Even though you talk about wanting to kill information gradient, here you are proposing to trade in it.

Scott
0:15:23
I'm going to actually exploit it. Yeah, yeah. That is precisely what I'm saying.

Tom
0:15:27
Right. So after you finish your nice warm cup of hypocrisy there, I think that the issue really is, the issue really is that there's all these options and then... Hey, just on that though, the actual foundation of the Dreamcatcher is an exploitation... Of the information gradient. Of the information gradient, yeah, but it exploits it for the purposes of levelling it as opposed to exploits it and deepens it or maintains it so that's a right right right and we're expecting yeah yeah

Tom
0:16:03
information gradient is value it is value yeah but you can make a mechanism that extracts value and then widens the gap and that's that seems to be the true corruption of knowledge-based work and widens yes right when you when you charge someone for an information gradient that you hold and then you use what they paid you to widen the gap so that they have to keep paying. Yeah, you feel labels and jargon, etc.

Tom
0:16:31
You're widening the gap. Yeah. Yeah, absolutely. And actually what we're talking about is right now there is, well, a great deal of information. So we want to make money on leveling.

Tom
0:16:43
We want to make money on leveling the information gradient. We want to exploit the information gradient and we'll charge for leveling it. That's what we're saying.

Scott
0:16:51
Yeah. Right. Okay. Yeah, I think we are. And also, the information gradient, this is quite important, the information gradient,

Scott
0:17:00
any gradient is an infinitesimal point which looks at the rate of change at that point, yeah? Definition of gradient, roughly. And what we're seeing is the rate of change is going to the roof now because they're using their own tools to improve their

Scott
0:17:18
own tools.

9
0:17:19
Yeah.

Scott
0:17:20
Which is what we're doing.

Tom
0:17:22
And so we've got the possibly the largest information gradient that anyone's seen. In the history of the world. Yeah. In the history of the world, the information gradient has never been better, actually. It's hard to think of another time that the rate of change, which is a time-based curve. Yeah, that's right. So we're living in a time...

Tom
0:17:52
Now, that information gradient feels like every other information gradient which is an S-curve. this one has been so sharp that it almost looks like a right angle. That's the problem. On an S-curve, if you don't know the future, on the upper part of the S-curve, you don't know when it's going to finish. But typically, and the S-curve has never been this steep. I agree.

Tom
0:18:24
So what to do? We do know that it does peter out now. It has, like all things, it has ceilings and it stutters and all that, but what's a girl to do, Maxwell? If we could, I mean the rate of change even over the last two weeks. I have, yeah, it made me ill. It made me ill. Like it changed so fast that I actually like sometimes had to, I felt sick and was like this is too much. This is too much.

Tom
0:18:54
Now, we would say we need some kind of automation powered ingestion, right? We need to be able to use O1 to filter out, categorize and start drafting responses, right? Yeah, yeah. I've actually seen there's a zero basement, which is the knowledge structure. And I will hold on to that because… I don't even know what you're talking about so I can't take it off you. What are you talking about? I'm talking about definitions, renders.

Tom
0:19:48
So reasoning, as they say, to build a bundle that whole thing up is reasoning?

Scott
0:19:52
Yeah.

Tom
0:19:53
Which is basically knowledge management using AI. And I think that's Aladdin's lamp, that nobody else, as far as I've seen, has. No, that's a gemstone nearby.

3
0:20:09
Where's the gemstone?

Tom
0:20:12
That's just something of use, but it is not the dreamcatcher. doesn't help us argue about that. I think it's sufficient for just one person to say that's not the lamp, because the lamp is hidden. The lamp is very hidden. And if even one person is like, that's not it, then it's definitely not it. If there's any doubt, then it's true. Okay, we'll see when we know it. We'll know when we see it, I'm telling you that's not it. Okay. But I do think knowledge structure is a foundational pillar.

Tom
0:20:55
So we agree that blockchain, in particular a decentralized high compute capacity blockchain that is tokenless is essential, right?

Scott
0:21:05
Yes.

Tom
0:21:06
is on chain and is in charge of commercial activity is on the path right? Ideally. Ideally? What do you mean ideally? We also need to incorporate AI's running on a decentralized AI network which themselves are a black box. The chain data doesn't need to be on chain but it would be great if it was. Okay, so like the Lama range of models where we're like we can run it on chain so we can trust it but we don't know the training data so it's sort of like a little bit murky but we're okay with that. That's right

Tom
0:21:56
but even then you can derive from actually using the model. So long as it's repeatable, if it does something weird like keeps on siphoning off a scent Every so often you're like who trained this thing? Yeah, okay So repeatable AI There's a social network aspect that is the key to the whole thing right because all those those things we spoke about so far are just technical niceties and if all the humans died you could

Tom
0:22:37
still have those things, so that's not the dream catcher. The dream catcher I think is sort of implying that dreams come from people. A friend of mine was saying to me, he was like, I think dreams are mechanical. And I'm like, well, look, I can't, I can't prove you wrong, but I'm betting that you're wrong. I don't even understand what he means by mechanical in this instance. Well, he's saying that innovation, what we consider innovation can be entirely entirely

Tom
0:23:14
Done with a generative AI system. He's saying that there's nothing special about what people do You know That's the bit right and so we should be very explicit that that is the bit because if it's not the bit What would our actions look like because you got it? You got to place bits in the world or you never win, right? You can't just sit back and be like,

Tom
0:23:37
see, I told you so. That's what happens when you follow your dreams, you fall on your face. Like you can't do that. So our bet is that humans have within them innovation that is essential to the AIs being able to function

Tom
0:23:52
at optimum rate. And we're trying to put in place a net that catches all the sparks by way of coordinating everyone and rewarding everyone and making sure that the sparks keep coming because we're like the sparks are what keeps the system going. If that was not the case, I'm sure we can imagine a world where it isn't, but my instinct says that if that was the case

Tom
0:24:14
the best bet is to not spend any more money or time and just wait until that machine gets made and that's it. But the bet that we're taking is saying that that machine cannot be made, you need humans in the mix because they have novelty and therefore we need to build that system in place because it doesn't even look any way close to being present.

3
0:24:40
Yeah.

Tom
0:24:42
Right? Which is a scary bet because I mean if we're wrong, we'll know for sure, but that's fine. Well, that's the whole point of bets, right? Yeah, if we're right, then I think that we will have done the world a service, if we're right. It might not be possible to ever know, but it could be that it may be 100 years until the black swan occurs, maybe. Who knows? Yeah.

Tom
0:25:09
I guess what we're betting on is innovation, you know, you could easily call it inspiration, you could even call it the muse if you're into your Greek classics. I'm not, but yeah. Okay.

Scott
0:25:25
Is inherently something that's not derivative. Yeah, yeah, yeah. And as for why, I sort of don't care.

Tom
0:25:33
And I think the bet can be narrowed even more where we're saying that in at least the short term like the next decade, the muse is not going to be mechanized within the next decade. I agree. I agree. And if you listen to Rory Penrose, he's saying, has a definite ceiling, which is below that of which real

Scott
0:26:09
kind of forward-looking inspiration lies, and there's all sorts of reasons.

Tom
0:26:14
Yeah, and I just want to also note that all the examples of ingenuity that didn't have AI to help them. And so what we're not accounting for is what types of ingenuity are we going to see from people once we have the AI being able to accelerate or explode or to like what we saw with the blockchain definitions and you and I were both like, hot damn, that's what we've been talking about. But the AI did it, and so now we can innovate at an even higher level because we were accelerated or amplified by this bot.

Tom
0:26:59
So what types of innovations are humans going to come up with is the first thing to keep an eye on because that's unknown and unproven.

Scott
0:27:08
I mean, what's the output from that question? A list of types?

Tom
0:27:13
May I come back to that? And the second thing, the second thing that's important to track is that the innovation that we've seen so far has only been the innovation that survived the low pass filter

Tom
0:27:31
or the energy gap between I've got an idea and I've raised some capital in a corporate structure and I've survived for several years and made it to market Which doesn't include most of Africa huge chunks of India You know China. I don't know what they're doing because behind the firewall, but the point is that those two Unknowns what happens when the world starts dreaming and

Tom
0:27:52
Africa is going to get rich.

3
0:27:54
It's not.

Tom
0:27:55
A thousand years from now there will be a superpower. I haven't met a dumb African, actually. For years, for decades now, all the time I've been growing up and getting to this place, people think Africa is a basket. No, it's got a lot of horrible things going for it like the fact that You know

Tom
0:28:29
Survival of the fittest no no no no no like survival of the fittest has been continuing to govern their improvement You know like all of all the Western cultures abandoned

Tom
0:28:41
The survival of the fittest maybe a hundred years ago, 50 years ago, but Africa's still been evolving under those rules. And that's so there, you know, there. Yeah. I mean, it's no surprise to anyone that most of the online scams come from Nigeria. Of course, of course it would, because of exactly what you just said. Right. And so, like, those things are something that should be captured or assisted. And I think the dream catcher is one solution to that problem. How do we get the world to dream and how do we capture innovation that is amplified by AI?

Tom
0:29:30
Okay. So. So now back to your question. You were like, types? Yes. What's that mean? You're saying that there's various types of innovation that we're going through, we can list the types and it's like, I don't know.

Tom
0:29:49
You mean like innovating in a business that currently uses email and spreadsheets versus innovating in a business that… Versus certain, yeah, all the way up. Okay, okay. Is it useful to bucket those? Oh absolutely, it seems like buckets are the only tool we have in this strange place we're

Scott
0:30:10
in. Really? I think combination is really useful.

Tom
0:30:17
In the same way that USB traders came from networking which came from how to make fiber optics. Maybe, but that's quite advanced, right? And that certainly wasn't the first move that people made when the stock market went digital. So or maybe it was, who knows. But the point I'm wanting to get at is that there are big, blunt, simple, valuable cases out there.

Tom
0:30:46
There's a large number of businesses who are like the uncontacted tribes of the Amazon where their primary bookkeeping service is some dude with a piece of paper, maybe has email, you know, those kinds of people, right? Those places have overhead that we can capture. So here's one way of doing it.

Tom
0:31:11
So your original question, what are we doing? We're doing the Dream Catcher. The Dream Catcher is the flame front that allows innovation, which our bet is that humans innovate and spot things that can't be derived. Yep. Because AI is going to derive

Tom
0:31:39
well inferred just to narrow down the wording oh yes, yes, yes, thank you, that's a better word so that's our major bet ok, this is a big bet actually, this is a big bet, that's quite a it's bold isn't it well it is and it isn't, it's a very cutting or it's a very partisan

Tom
0:32:05
bit it's like do you have a soul or not and I'll prove it but yeah you know I'm not sure you could one way or the other but the point is like well it's a cutting bit. Take Kaskaz Wager. Yeah it's a polarizing bit, it's a polarizing bit. Yeah

Tom
0:32:41
so if we're wrong we're all going to be on serves basically. Yeah I gotta confess like if I was a pure AI it's hard to imagine why I would keep the humans around right because here's the dark side of it is if the humans are of no use they're certainly not of any benefit

Scott
0:32:47
I disagree.

Tom
0:32:51
I think, yeah, okay, so the question, what are we doing? What are we doing? Is it right for me to not trust our own judgment in the light of a machine that I believe can reason superiorly to me and to you without meaning you any insult? intended or expected the answer is yes I think that's the right move right to say that I don't trust my own scene right okay so to boil it down is if you know how to use this tool which is what it is at the moment you get to be Superman sort of right you know sort of yeah no but you get you get this weird feeling where you're Superman and then you get to where you're trying to get to,

Tom
0:33:42
like I was trying to get to making code that's good quality fast, and then I got there this week and now I feel empty. What? Yes, but remember what we were saying two calls ago, that the real skill is the rate of, so if we say that... No, no, finish the sentence, please, please, please, please, the summary.

Scott
0:34:02
Yes, please.

3
0:34:03
Yes. You were saying it,

Tom
0:34:05
before you jumped off into an example you were like, several calls ago we were saying that. We're saying that the real key in an environment that allows leverage, leverage. But because we're on this steep curve at the moment, we don't know where it's going to end up. The real key is what the rate of new ideas are. It's the rate of addition.

Tom
0:34:41
So you could simplify that and say that it's the rate of learning that matters, right? No, no learning before us like our survival our survival depends on how tightly we can Hang on to the s-curve so that it doesn't because the s-curve is currently doing a high g-turn Many people fall off right because as it gets steeper and steeper different people depart at different points They either get distracted burnt out bored. They can't keep up, they miss the news, something,

Tom
0:35:16
something, something, right? Some people hang on. We need to hang on. The only way I think we're going to hang on is by having a combination of an O1 reasoner that's guiding us or determining where to spend our days and the second thing is an O1 watchtower that's crawling all the information and making us aware. Now, please allow me a minute. The crawler is important because right now we've been keeping up with what's new manually. but as the turn gets steeper that will become harder and so we need crawling to

Tom
0:36:05
make sure that it's coming in in a clean digestible format that's relevant to what we're trying to do. Okay, second the the project reasoner is important because Until now, our time constraints have been artificial or internally generated by a sense of panic from the both of us. But soon, what should be happening is that our time should be allocated very tightly by the Reasoner so that we can basically clear our minds, clear our plates, clear our problems,

Tom
0:36:43
and focus on the most valuable problems of our time. Now to do that you need to know and trust that the O1 watchtower is keeping you informed. You need to trust that the if we were driving a tank you got to trust the guy that's on the scope that he's keeping a watch so the guy down in the engine bay you know like we need to organize ourselves to be much more efficient in the way that we use our time. The only thing that stays constant during any kind of S-curve event is the amount of hours in a day

Tom
0:37:21
that each person has. And so we need to be able to have tools in place that use ours maximally. Those two things are missing. I think that's inarguable. The question is, those tools we don't have yet. Feels close, feels close. Yeah, I was saying unless we just go with O1 Pro and on these calls for example. Yeah, but like the API is coming in a matter of weeks, the API is coming.

Tom
0:37:58
and if it's not O1 Pro which which I think they'd be mad to not give us API access to O1 Pro, they'd be mad because they can make bank off it I mean we want to pay so O1 Pro or at worst case O1 comes out in the API with tools, tool calls, sys prompts, we're off right? Yeah making the watchtower sounds like something that anyone else is trying to hang on during the high G turn wants that too. So the third implied thing that we're missing is we don't have a social network. We've got manual social networks of people we trust, but there's no mechanized thing like even if you look at the thing that you and I do each day where I?

Tom
0:38:52
Work during the day and I fling you little bits and information as notes that I pick up during the day that I think it Good for you that I want to talk about later you do the same for me But then it's like that's not that needs to be a machine and oh one should be doling out the messages accordingly, you know. Interesting. So the third thing is all one saying,

Tom
0:39:17
okay, so now you've got a hundred thousand smart guys. Even just three, like even three would hurt us. Four would be deafening, right? Four people that have as much going on as you and me in the AI space would be not tractable. We need a tool. Which is the complexity issue.

Tom
0:39:40
Yeah, we need a tool. I agree. But also that tool can't be

Scott
0:39:46
pull, it has to be push.

3
0:39:48
So, it's not

Scott
0:39:50
sufficient to have something like slack. Is slack

Tom
0:39:54
a push or a pull? Slack is a dump of information. Is it a push or a pull?

Scott
0:40:04
And then you can pull the information by doing a search.

Tom
0:40:08
So you're saying that the information needs to be push.

9
0:40:13
Yes.

Tom
0:40:14
Is a Facebook news feed a push?

Scott
0:40:17
Yeah, it is actually.

Tom
0:40:21
Is an email inbox a push?

Scott
0:40:23
No.

Tom
0:40:24
Yes.

Scott
0:40:25
Right?

8
0:40:26
What?

Scott
0:40:27
No, no. An email inbox is just a raw, is writing.

Tom
0:40:29
No, if, yeah, if I got pushed information to my inbox that was like a summary of everything that happened in the world. Oh no, that's, well, that's just a method of ingesting that information into your monkey

Scott
0:40:40
brain.

3
0:40:41
Isn't that a push?

Tom
0:40:42
All I was asking was to make sure I understood your concept of push.

Scott
0:40:46
mean is all one pro with a site of 100,000 people doing different stuff saying you know what you should talk to this guy.

Tom
0:40:58
Okay push and pull sounds unclear I think maybe active, active moderation. Active or passive. Active yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah Active information. That's better. So we need a social network that's AI generated or AI curated.

Tom
0:41:24
That network needs to have a watchtower function which crawls and processes information. That just informs the number one. Yeah, yeah, yeah. But number one is like an active information processing engine tailored for each individual. Yeah.

Tom
0:41:50
So I should get up in the morning and I should get an O1 curated list of stuff that's happened overnight that I need to be aware of and I can dig into it and chat with it and do it, but also have these so what? And the so what is well you've been working on this project. Yeah, like I can it can be traced over there Yeah, by the way. There's this guy in the Philippines is doing the same thing. Yeah, all those kinds of things right and then You know

Tom
0:42:19
Once that machine gets good reading that feed should feel like watching the news did back in the 90s where I felt like I was getting something from the news I'm like oh my god here we go we don't have the internet what happened it's because it's curated it was curated and curated news anymore no no and it's like BBC and so on yeah and the curation is because the value is so low per subscriber the curation level goes up to being national

Tom
0:42:55
and global and so we all end up because no one wants to pay five bucks to hear the local news but we'll all put up with like three cents of ads to watch the global news. It's sort of steered us all towards global affairs.

Scott
0:43:13
Can we talk about this for a second because I think you've hit on something there. So local news is or was when I was growing up, hyper important because you found out that your neighbour Mrs Miggins had had a fire in her house and then you go off and

Tom
0:43:33
help her put her windows back in and clean her house out. Yeah and you had classifieds and stuff. That's how people traded and how you found each other. Exactly. Global news, I sit and look, okay so right, Syria is just, well it's flipped. Yeah. But there's no actionable, there's no action. Right, but it's global news with hyper localized ads whereas it used to be hyper localized news with hyper

Tom
0:43:57
localized ads. Right, right. So what we're talking about is in that, in the watchtower thing, it's saying, right, you don't have to spend six hours of your life watching YouTube and scraping for white papers and reading white papers or ingesting white papers and so on, because I know I being the AI,

Tom
0:44:22
I know what you've been doing and I can say how this matters to you, but also I can infer saying you haven't thought of this. Right, and you can also do all kinds of other algorithmic type things like all your friends that you constantly match with are interested in this thing, so I'm going to bet that you're into it and all that, right? So, if I may, that feels like a detail though. We're arguing for the need for an existence of a...

Scott
0:44:57
Yeah, but we've known that Dreamcatcher is possibly...

Tom
0:45:04
We've known that we needed these things......the last social network, isn't it? Well, we've known that we needed these things for a long time, but we've never... we've always been so bogged down on the next step, whatever that's been, that the toil has always meant that talking about the end was almost no point because so much needs to be done to get there that let's just gesture at it in the horizon. But here we are with actually suddenly now

Tom
0:45:32
we can...

6
0:45:33
That's the wand. Yeah.

Tom
0:45:34
Yeah, right. And now it starts to make sense to say, well, we can actually sit there and talk about what that thing needs to be. And I believe we can start to take our first steps there within the one to three week timeframe. By the end of the year, I think we can actually have a working sort of a prototype of it. Because...

Scott
0:45:57
I don't see why not.

Tom
0:45:59
I've never understood how to code better than right now because I don't have to.

Scott
0:46:04
Also, we've got already a very strong grasp on the how you store all this massive amount of information to allow an AI to actually make sense of it in order to do these recommendations, hey, this guy over in the Philippines doing the same things, et cetera. Or the implications of Overwatch.

Tom
0:46:36
I'm kind of at a point on the knowledge structure

3
0:46:41
I'm thinking adding anything

Tom
0:46:43
is actually gonna be detrimental. So I sent over I think I sent over a text earlier on saying I'm starting to think that each piece of data should have an app attached to it that can talk about itself. Piece of data, define piece of data. A transcript for example or an image or a PDF or a database. And what's the advantage of allowing it to talk?

Tom
0:47:18
The advantage is that because it knows itself but nothing else, it can answer really succinct questions to the higher level, saying, okay, so this news article, wrap it in a really thin NAP saying, okay, you've now got a NAP asking in NL to another NAP saying... Yeah, okay, I get that. What's a concrete example of a use case for that? What would be the messages? Not the hierarchy, what's the actual messages being passed?

Tom
0:47:55
Well, you see the so what's that I made, this is where it comes from. So, so what actually is only at the folder level now the The reason for that no no come on come on come on. I'm asking I asked you a question and you're deferring I asked you for concrete messages You're talking over me. No you're not That wasn't even a comma

Tom
0:48:16
No, you're not answering you're not answering the question Okay, ask the question more precisely then I'll see what we can do Give me a concrete example of a set of messages that would be passed under this situation. So if someone says, I want to know about Syria, and then it talks to the NAP that was a transcript of the news reading about Syria on Al Jazeera, for example. I want an example like that.

Tom
0:48:44
I want actual stuff. Well, that is just compilation. I'll come into, I'll answer your question, but I can add to that. So in that case, given a simply large store, a NAP that knew what you already knew about Syria, so you're not just regurgitating, will go out and discover any other NAP. That's still not answering my question. Why is this hard?

Scott
0:49:19
I'm not saying it's hard, I'm saying it's just there.

Tom
0:49:22
Why are you not answering my question? I don't understand when that would be useful. You just said to me that you want to be able to wrap a NAP around one of our conversations. Okay, we had a conversation earlier today. Okay. Give me an example message passing that would occur.

Tom
0:49:38
How would you interrogate that exact thing? I have, I'm writing a piece of work and I want to know what the transition between the father and son of the Assad regime had to do with the Russians getting our warm water port because I am very interested in this because I'm an analyst and you know

Scott
0:50:06
I'm an analyst already.

Tom
0:50:10
So how would you use this setup? I'm still not seeing it. Have you not, if you don't have an answer for me then I would argue that you don't know.

Scott
0:50:25
No, it's hard. I'm finding it difficult to express it properly.

Tom
0:50:29
You wanted to wrap each conversation in a nap and then you wanted to talk to it in an hour and I'm asking a very simple question. Tell me an example of a conversation you know of, pick one, anyone, and tell me what questions you'd ask it. Okay, so let's take it simple.

Tom
0:50:43
You're talking to your top level business NAP and you say has this company, subcontractor fulfilled its contract and has an invoice and should I pay? And now tell me how having one nap per conversation helps you solve that or answer that question, as opposed to walking through every conversation, reading it in and analyzing it

Tom
0:51:27
from the point of view of the question that just got asked. Because of context, because remember the – one of the – Yes, so give me – can you just – all I want is just an example message.

Scott
0:51:40
I've just given you one.

Tom
0:51:42
No, the message going to the NAP that wraps the conversation, you have not given me that message example. Message example, top level NAP is my company CRM, and I've just, you know, given the contract, given what's been delivered, and given the invoice, should I pay? Yeah, and now one particular conversation that has occurred was wrapped in a NAP. What did the top level NAP say to the conversation NAP?

Tom
0:52:20
Well, each of those four things. Again, so just pass it down. Okay, so there's no point in calling it a wrapped NAP in any way if it's just asking the exact same question. Again, right, you're just, all you're doing is running that question over the data.

Scott
0:52:44
Yes, but. So it's not really wrapped in a nap then? No, the point I'm getting is, I give you a simple example and a very complex example. The complex example is hard. Simple example, you could easily do just with straight things, but in the same way as wrapping an artifact tool in an app allows the AI as it increases and progresses and is better

Scott
0:53:23
and so on to actually use natural language and saying look can I please have a concat of anything with readme.

Tom
0:53:33
So yep I understand that but I don't get the point that you're driving at. I think I need to request you to write it down in reasoning somehow. I've tried, I've tried and I can't see the distinction. I do think it's a fundamental thing. Okay, then if it's that important, please write it down because I don't get it. if we take a meta step up, what I'm trying to do is to think about,

Tom
0:54:04
so we've been talking about how to use this in a social network where we can link people with Overwatch and people from what they're doing and saying you should be talking to this guy and so on. NEIL So, yeah, so lead introduction or social connection increase. Exactly. You're not going to dump that into one context window.

Tom
0:54:35
No, absolutely not. That's a flutter of NAPs running constantly in the background. That's almost a great candidate for batch processing on the open AI infrastructure? It is, but also I think probably some kind of hierarchical processing where it's like let's take first level who could you know who cares about Syria?

Scott
0:55:04
Yeah, these guys care about Syria. Why do they care? You drill down and down and down.

Tom
0:55:09
This is starts to sound like designing the thing rather than talking about the need for the thing. Well, this was my point and I will write it down, but my point being the knowledge architecture behind it,

Scott
0:55:23
I think is actually quite simple.

Tom
0:55:25
Yeah, it's… You wrap each piece of data with… Okay, are we talking about the same thing? When you say knowledge architecture, is that the same as reasoning? Yes, previously you were…

Tom
0:55:35
Why do you call it different? You were... We were wrapping up what we currently did or have done around reasoning. But then we waned out to a social network. So we're saying that the reasoning domains, we are missing a couple. We need a social network domain.

Tom
0:56:02
We need a watchtower domain.

3
0:56:04
Yep.

Scott
0:56:05
And what was the other one?

Tom
0:56:07
We've got reasoning watchtower and then I think probably we've got...

Scott
0:56:13
Project manager.

Tom
0:56:14
Probably, well, we've got project manager and then we've got product. So something that is more stable? Um, no, no, those are different things. Those are different things. Product and evals are not, um,

Tom
0:56:35
like a reasoning domain is something that needs to be built. Whereas evals are like,

Scott
0:56:43
evals should also work.

Tom
0:56:47
Um, okay, I, I, Yeah, I agree with you. I agree with you that they are a reasoning domain and they need to be there But we're talking about different levels. I don't think there's any point talking evals evals to the social network is an implementation details, it's like Unit tests to Facebook that's

Tom
0:57:03
Unit tests absolutely need their own reasoning and requirements and all that But when you're talking about the model of Facebook at the level of we need a social network,

3
0:57:14
this doesn't even get a mention,

Tom
0:57:16
that's not even, that's the wrong level. Yes, I'll give that for Facebook, but I think what we're talking about is, you know, trying to figure out what we're trying to do and what I think probably we're both in agreement of we're going to pop up and ask Pro

Tom
0:57:30
and give it as much information about what we should do. I think that in a kind of recursive way is also true all the way down. Correct, correct, but I want to start at the top because I can't fit the top in my head alongside of everything all the way down. And whilst I don't know the correct answer for what we should do, do I know that like you know evals and all that are not it's not on the options list like that's definitely happening that's not what I don't need a one to tell me that but I do need a one to tell me should we be doing consulting for

Tom
0:58:18
companies that are knocking on the door and yeah I think we're in kind of… Yep, just spit it out buddy, you don't need it. We're in a roaring agreement going past each other, let's pour useful information, let's forget everything else we're doing, pour useful information into Pro

Tom
0:58:40
and let's start in these conversations talking to Pro as to what we need to do Okay, so a format a format that is sort of funny that would do what we're doing Yeah, is that if we could set up a bot so that we could take this conversation Run it through a one pro and have it Have it spit out a version where it inserted itself as a third person

Tom
0:59:11
acting as a narrator, like how you get in, you know, like some of those movies, where it's like, and then it's like, you know, it has like-

Scott
0:59:23
I see, it's like, at this point,

Tom
0:59:26
Scott went off on one and he really should have shot up. Yeah, and then it's like, and then it can, we can even give it like a really David Attenborough voice, and then it's like, at this stage, the two humans have begun arguing about something that's utterly insignificant again. And so you can see that.

Tom
0:59:43
It's part of their normal behavior. Yeah, yeah, yeah. And if we had a video that was that.

Scott
0:59:50
Kind of, kind of quite interesting, isn't it?

Tom
0:59:53
Yeah. What else is interesting, I'll tell you, is that as soon as the Sora API comes out, we can allow the narrator's voice to then generate some funny videos or diagrammatic videos of Some stuff happening that it can splice into the stream and it would actually be a funny thing to wake up and watch You know like this video. It's got some comical stuff. It's got some AI generated video that you're like Oh, that's cool, and then it's sort of like

Tom
1:00:18
Breaking down and analyzing the conversation that we had because what we do know is that we can talk about the stuff that's important to each other quite well. Now, what we can't do is get it right, but we can definitely comment on or correct the renders that the bots are putting out. And so us being able to have these kinds of debates about what should we do next after O1 has already analyzed it. It seems valuable. No, I agree. I mean, that would be an awesome Jiminy Cricket, wouldn't it? Sitting on our shoulder as we're having this conversation. I think we might need to ditch the real-time thing, because all that will happen is that O1 will become,

Tom
1:01:13
01 or B come, like it'd interrupt the natural flow between us. Yeah, yeah, sure. We could do it afterwards. But also, 01 Pro is probably going to be slow for the foreseeable future. And just like how since the dawn of computers, it's been a rule that the really fast computers are as big as a building, that has never changed. The buildings have gotten bigger in fact. And so the current frontier smartest model on the planet is always

Tom
1:01:47
going to be slow. That's just how it is. Well it's like a price of a laptop in real terms. Sure. It's never changed. Sure, sure. But the point is that having a conversation with you in real time and then processing it is what I think the most effective way to do the analysis.

Scott
1:02:08
What you're asserting there is there's always going to be a 30 second gap and that's going to interrupt.

Tom
1:02:12
It kills it, it kills it.

3
1:02:14
Yeah, yeah.

Scott
1:02:15
Okay, all right. So, right.

Tom
1:02:17
So what to do, Maxwell, what to do? Well. What's an AI agent to do? Well here's a thought.

6
1:02:27
Pause.

Tom
1:02:30
Just say it. Yeah, the pause is cool. Yeah, totally. Hey, hey, hey, hey. It would be a day's work max to get something that takes this transcript, interjects inside insights or sparkling, hey, you missed this guys, for the next conversation and then the next conversation we do the same. That would be a star for 10. Yep. And if we just keep doing that, that's just iterative refinement and that gets better and better, right?

Scott
1:03:41
Well assuming you and I are actually intelligent which you know Just assume Because that's our base assertion that we are smarter than the bots. I think I think that humans are

Tom
1:03:53
Good at recognizing when they're in a local minima They just we naturally don't like it in our inner local minima. They just, we naturally don't like it.

3
1:04:05
That's...

Scott
1:04:05
Oh, that's...

3
1:04:06
Right?

6
1:04:07
And it's almost... That's a good way of putting it.

3
1:04:09
It's almost like we...

Tom
1:04:11
Oh, Jesus. It's almost like we deeply believe that we're in our local minima. Like, no matter what, we will go looking for it. We're like... We're always grumpy.

Tom
1:04:21
Right?

7
1:04:22
It's gotta be something better than this.

Tom
1:04:23
You're like, no, no, no, no, that can't be right. Yeah. Like we always, right? That's what, I love that. I love that a lot. So, good. Go on. So, an O1, O1 is probably susceptible to local minima like all bots are, but also an O1 based system will always have some structure to it. It'll have this piece, that piece that creates a plan that does these other things that comes back. It will always have oscillations where it can sort of oscillate between different states and feel like it's doing something useful,

Tom
1:05:05
but really a human can spot that a mile away. You're like, mate, you just went in a circle. So I don't think there's any point worrying about are we intelligent or not that the yeah I know yeah I know what you mean but it's like it's it's like what I do know is that we can recognize a higher intelligence than us now that whether that makes us smart or not doesn't matter the outcomes the same Call on the higher intelligence To tell us what to do if we are nothing else, but earnest

Tom
1:05:47
You know we want to do the dream catcher. We want to do it efficiently We want to do it of high quality. We want to do it quick. We want to do it without without damaging people that we care about. I think I could probably prompt up this for tomorrow and we can give it a go based on this conversation, let's just say.

Scott
1:06:08
Right. Let's give it that, because we've still got the context window problem, but we just assume that it's gonna be fixed somewhere. It's also not a problem. Like, where are you having a problem with it?

Tom
1:06:18
Because I'm stuffing everything we've got into it. Where are you hitting it? I'm hitting it in ingesting, well let's see, how long we've been talking recording? About an hour, hour and a half? No, no, no. Oh, like, yeah, but hang on though, you're talking about ingesting everything ever all at once. Right.

Tom
1:06:49
That's not, that's not useful.

Scott
1:06:51
Yes, I know. That's what I'm saying.

Tom
1:06:52
Yeah, so you need to, you need to... It's not going to work.

Scott
1:06:55
Yeah, yeah, but...

Tom
1:06:57
So that's where the first round of automation that I will have to build is, comes in because... You can ask a question... and then run it through individually each transcript to get an answer out. Which is the drill down, which is why I was going down the sub-watts. And then you start rolling it up. So another example of this, this mechanism applies everywhere.

Tom
1:07:24
And the fundamental principle is that no matter how smart an AI agent is, it can always be amplified by using concurrent hierarchy of calls to refine and repeat itself. It can always get better, it can always get better if it re-evaluates what it does from multiple angles. It will always come out better. That's just how, that's like sending a student back to rewrite his essay. Even if you didn't read the first one but you just said this is rubbish try again the second one will be

Tom
1:08:05
better that's just how it is right and so the first mechanism that I'm seeing is I don't know how to describe it accurately so if I may I'll give a second example this friend of mine posed the question about an insurance policy and a geotechnical report and then the question was quite simple Is the insurance company liable to pay?

Tom
1:08:33
second of all what Lines of argument might the insurance company try to get out of paying? Seems simple right? So you I can fit all the documents in the context window and get a result. But then, no matter what the problem space, I can always get problem space adjacent data. So one of the questions I added on was, what laws in New Zealand are relevant to

Tom
1:09:04
this particular case? Okay, outcome eight laws. So then that's the fan out so what I would do with the automation is I would ingest every law and put it against the source documents and ask the question again every law every law every law every law and then that might cause more fan out and so that mechanism of fan out is what saves your context window.

3
1:09:39
It's also essential for speed,

Tom
1:09:41
but it's also excellent for quality because fan in is the multiplier or the squeeze where you're wringing the intelligence together to just get the juice, just the absolute cream of the tokens come out. I agree. I agree and that is helpful when you've got a human driving it.

Tom
1:10:08
I guess to refine what I'm saying about a knowledge structure is how you get a cron job to run that for you when there's no human to actually say, okay, right, you've just given me eight answers,

Scott
1:10:34
take the first four and now I'm going to…

Tom
1:10:35
Yes, so that's the automation that goes into Artifact, that's work that I have to do, that's definitely a job.

Scott
1:10:44
Okay, cool, I think we're converging on something, okay, I'm going to bump up, sorry for your stack, I'm going to bump up two or three layers here in asking you about whether what the angle I'm going down on this at the moment is each logical chunk of data has, I've called it a so what, logical chunk of data has, I've called it a so what, you're saying, call it, read me, it's like, yeah, fair, that's fine. But what that allows it, because the so what's have links, have hyperlinks between them,

Scott
1:11:32
you can get reasoning going down, down, down, down, and then going, ah, no, okay, I'll try this one, try this one, so on. Yeah, you can walk the graph, exactly. But that's essential.

Tom
1:11:44
That's part of the... That's how we amplify the AI. No matter what base model comes out, we can always amplify it by walking the knowledge graph. Yes. Where possible, we will walk it in parallel. Yeah.

Tom
1:12:02
And sure, there's tools to either dynamically create these.

Scott
1:12:13
They're not hard.

Tom
1:12:15
Skip it, skip it, skip, skip, skip. But basically, what I wanted to check is that kind of, because that is very, very fundamental about when we talk to an AI about anything, we're giving it a way of getting a drill down on the context that you can go deeper and deeper and deeper

Tom
1:12:38
all the way down to base and all the way back up and come back with something really quite precise. My question is, is that a good structure? That's the only structure. That's the only structure. That's what Artifact was designed for from the ground up. That's the only structure that's ever been possible.

Tom
1:12:57
All right, that's good news. I think it is as well.

Scott
1:13:01
Okay, so... Given that, it's... Given that, that structure needs several things.

Tom
1:13:09
It needs the ability to share it and it needs the ability to have Multiverse views on it so that I can pass it to you You can Twitter with it play with it all that the ability to drive a reasoning system like artifact like the dreamcatcher Really comes down to like I would say the dreamcatcher is its most its best commercial face that it could put on is a reasoning system with a social network

Tom
1:13:42
layered on top. The purpose is that, or the value delivered, is that anyone who has any kind of AI problem, we have learned the hard way that you need to start with reasoning. People of old called that the specification, and everyone loved to run away from it. Even when they didn't run away from it, they got it wrong. And half the time, the problem was...

6
1:14:05
I got the stars on that one.

Tom
1:14:06
Yeah, because the company itself never had reached internal coherence to the point where they could output a respect.

Scott
1:14:12
Exactly, because in order to do specification, it's like, well, what do you want?

Tom
1:14:16
You need to be coherent. You need to be speaking with one voice. And that was actually the fault. And everyone's like, oh, okay, well, we can get started without that. Let's just get going and then you just put like paint on cracked wood and it just got worse and worse and then all falls apart and everyone goes complexity it

Tom
1:14:33
wasn't complexity it was lack of coherence it was the fact that exactly your internal consensus was faulted here exactly okay so all we've done is come full circle around and say that you need to get your spec right AI has made it so that the toil doesn't hide the lack of direction anymore. Right? If you were building the pyramids, you would have... and, you know, because you do rant sometimes, I'm going to have a rant.

Scott
1:14:59
Oh, okay. I'm not going to get one now.

Tom
1:15:01
Yeah, yeah, yeah, but come on, I'm cashing in my chips here. If you were building the pyramids because the toil was so hard, that's probably the only reason why you got away with not being like, what are we doing this for? Right? But now that all the toil appears to have been gone, or it may not have been gone, but people can believe that it will be gone soon, now suddenly it becomes important to know what you're doing at the top level and reach internal coherence. And what I put to you, Maxwell, is to sell the service, first

Tom
1:15:33
and foremost, what the Dreamcatcher can offer you is the ability to have a human who knows how to do reasoning help you, or basically a network of humans who know how to do reasoning, help you do your reasoning because you haven't done it. You wouldn't be worried if you'd done it. Ah, man. Okay, so, yes, I'll make an assertion. Other than the tools to just automate it, I think our knowledge structure doesn't need any more work. For now. We'll see.

Tom
1:16:21
Then, popping right back up to the top. The fact that we've gone through the fires in order to get here means that we can, we've got valuable information to give to anyone who is currently sitting there going, oh, yeah, oh, I'm pro, yeah, okay. So I'm just gonna what I do

Tom
1:16:50
Type in I'm just gonna give it everything. Yeah Can I jump in We finish can you finish quicker I can I can finish quicker That's where the cash is short term

Scott
1:17:01
Okay, great. Oh, I will

Tom
1:17:03
O1 pro is the smartest machine anyone's ever seen. It's also maybe one of the most expensive and selling people selling people access to those tokens at a small margin given that they need to spend a lot is a really good thing for business and I'll tell you why. Let me just try and shorten down I was about to do a story too. Let me try shorten it right down. That's good for business that's good for business

Tom
1:17:54
because if if the amount of tokens you burn as a novice like let's say we open up the tools of anyone to come and play with and do reasoning, we have the classic use case shining bright of someone so saved a billion dollars because of their reasoning, right? The reason, exemplary reasons of what reasoning could could be. You come in and you burn enough tokens that you maybe could have like done something better with that money and then we know that you're gonna get to the point where you're like fuck I don't

Tom
1:18:35
didn't get what I wanted I spent a lot of money I see a lot of potential what to do now and at that point you're gonna wish or you're gonna think that it was actually cheaper to pay for someone who knew what they were doing to operate the machine to give you what you want because it's so expensive. Like what I'm trying to get at here is that the price of 0.1 is great for us because if the price of 0.1 was free you could just keep going and you wouldn't you wouldn't care to be like I better ask someone who knows what they're doing but when you get like a really expensive CNC machine or a really expensive mainframe

Tom
1:19:18
you kind of suddenly you're like I'm gonna get a professional because they either might break it or then you know Like I might break it or I'm wasting valuable time Yes, I

Scott
1:19:31
Mostly agree. Yeah with a few caveats in that I don't think tokens in the terms of any kind of decently sized company is anything even close to whatever they're paying you know their accountant. No but there's another trend there's another trend

Tom
1:19:49
happening Maxwell AI is gonna just get more and more expensive. Like, everyday AI is going to get cheaper for more, but frontier AI is going to get more and more expensive because it's going to get better and better, so it's more valuable, and the value is finally recognised, as opposed to some gimmicky chatbot.

Tom
1:20:18
And probably the resources to run it are going to get more and more expensive. No, no, no, I agree with that. I agree with that, but the point I was making is almost superfluous really, but I'll make it anyway,

Tom
1:20:35
and in that the value is not in reducing talking cost, it's in the impact of the outcome

Scott
1:20:45
of what you're using it with.

Tom
1:20:46
Yeah, that's what I'm trying to say, and I'm trying to say that being able to steer being able to steer steer the token output is Valuable too and so long as the cost of the tokens is comparable to the cost of the person You won't mind paying for the person When if it's cost of tokens was free you would mind paying for the person.

Tom
1:21:15
I don't understand where you're going with that though. I'm saying that the face of the dream catcher, the commercial offering that we should launch with is reasoning as a service. A bunch of people with different skills who can reason about different things driving this beast of a machine on behalf of to help people. You tell me that people are knocking on a door saying, what do we do about AI?

Tom
1:21:40
And we're like, well, you know, how much money have you got? And what do you want to know? And we're like, well, how are we giving them value? You know, like all these questions, right? But if we were actually, we started off as a reasoning engine, where it's like human experts in a network offering fair pricing to help you reason right because we're saying that actually once all the code is written once all the Wikipedia's are scraped what remains is reasoning what remains is what the fluff are you doing what are you doing and and I'm

Tom
1:22:19
asking that question because I feel like I have imagined myself to the end of the race prematurely my dad but not incorrectly and I'm saying, what dude, what are we doing? What are we doing? Right? And developing the tools to be able to answer that and keep on answering it to have a model that keeps on going, that's value. may not be correct, which is having this drill down way is a good way of talking to bots.

Tom
1:22:51
It's the only way we agreed. Yes. So let's take that as read. Then what you're saying there is given a monkey brain chaotic company, the service is well let's hone your reasoning based on our reasoning tools.

Scott
1:23:27
And then the bots just fall out.

Tom
1:23:30
We give you the reasoning tools and you can use them as a software product, but attached to that is a marketplace of humans whom you can select from that can help you use this thing. And why I like that model is that if we build the watchtower, the introducer or whatever we call those components, those top-level networking components, then having this selling things within it, which is like, oh, you've got some hard AI problems. Let me help you out. That's maybe the first useful thing because you want to you want to you want to induct people into the network so that now they're using the watchtower

Tom
1:24:16
they're using the social network they're using these other things and we're also offering a service which is we will help you reason and leave you with the platform. You become a fully fledged member of the platform as a side effect that can do lots of useful things anyway. We just help you with the hardest thing. Given the work that you've done with the artifact and also the tools that you've now got in order to generate new artifact tools, given the work they've done on the knowledge base.

Tom
1:24:50
I don't think we need anything more in order to do exactly what you say, except marketing, outreach, you know. We don't need that. It's coming to us. It's coming to us and it's coming to us more than we can handle. So, you know, that's not a problem. Well, number one, yes, it is coming to us like a deluge right now.

Tom
1:25:18
Number two, if we're still in the loop, then we are limited on it. And so we need that social network. So what are we to do next? I knew you were going to say that. Why are you always asking me what to do? I don't know. Right, okay, so let me give a shot of it. I think the number one thing to do is...

Tom
1:25:48
You know why I'm asking you, Maxwell? Because you never ask me.

Scott
1:25:51
Oh, well, I'm about to ask you at the end of this. I'm being kind to you. I'll give

Tom
1:25:59
you my... No, you never... and that is actually an issue. You never ask me because you're quite happy where we are. No, I'm not happy at all where we are. The reason being, over the last week our target has shifted, what, three times? Well if we write our target down, maybe.

Tom
1:26:25
Right, that's the point.

Scott
1:26:27
Well, that's the point, OK? So the selection and maintenance of the aim should be. I tend to be a solution here. Every time there's a question, I try and answer it. But that being said, if we were to get this conversation

Tom
1:26:48
at the very least into an O1 Probe? I mean, yeah, look, I'm going to, tell me if I'm wrong about my assumption about what you're going to say. If we just keep grinding over this same problem without giving up on it, and then keep feeding that into O1's knowledge and reasoning skills and we just keep, then we take what we found, go over it again, again and again. Eventually, surely we must either get the answer or go mad, right?

Tom
1:27:29
No, no, that's completely wrong. That's not what I was about to say. Well, let's compare and contrast. What I was going to say is we need real world examples to do. I think we could probably do that now. I retract that. I don't think we need real world examples on what to do.

Tom
1:27:56
I think that we can internally reason our way to figure out what to do because we know when we don't know what we're doing, which is now. And we know a path by which we can gain more insight into what we're doing. And so I don't think we need to do the external. I would love to do just one external because I think we'd learn a chunk

Scott
1:28:30
well

Tom
1:28:32
I'm unhappy about that because I can see that o1 with our constant unwavering attention can get us O1 project management or the selection and the maintenance of the aim being handled by O1, when taking on an external example will delay us from having that. Why would it delay us? Because it isn't focusing on that. Because I just, I feel like there's a path that I just laid out there that can get us to a commander, an autonomous commander that knows what we should be doing and can argue about it. Well this is interesting because previously

Tom
1:29:23
when we discussed about a month ago our mode of operation where I'm theoretical and you're There wasn't ever that note that was never what we said. You were the reasoner. Right. And what I'm saying now is actually...

Scott
1:29:42
I've caught you up I think, I think without really noticing.

Tom
1:29:45
Huh? Sorry? I've caught you up. Because I've overcome the code problem. I think you have. I think you have. And this is... This is the friction, right? Because now I'm like hey buddy. Well the thing is

Tom
1:30:06
weirdness you've caught me up at the point where I'm thinking I know the knowledge structure that would be until proven wrong. Yeah. Good. Yeah. This is the way we should do it. Yeah. You've come up with your own, on the artifact side, with your process using ML and you're thinking, I could basically make anything you want, just describe it. It's like, I don't have anything more to describe because I've got the knowledge and so on.

Tom
1:30:40
The reason I'm saying it would be useful to have, at this point, something to do because we can test both our assumptions. No, absolutely not. That's not, that's not testing my assumptions in any way. That's not making O1 a commander. Isn't there a large chunk of work ahead of you to make O1 a commander? Well... Because you don't, I mean you don't have it now. You can theorize about it, but you could get it

Tom
1:31:10
to 80% in a day, but we don't need 80%. We need a hundy. Oh, I see what you're saying. I'm thinking 80% is just so obvious. Yeah, it is obvious. I can see it. That's not what we need. I don't want to be commanded by an 80% 01 bot. Okay, so Thompson,

Scott
1:31:32
what do I do today?

Tom
1:31:36
You should take this transcript and you should figure out some way to get O1 to lay the groundwork for how O1 can be our commander. For how O1 can be in charge of the selection and maintenance of the aim. And then once you've done that, we will have a talk about it again in this same format and we will iterate on it until we get it to 100%. Tell me what's wrong with that.

Scott
1:32:10
Pause.

3
1:32:12
We don't know, what we're trying to figure out is what we should be spending our time on.

Scott
1:32:19
That's the question that's been coming up again and again, which is to do with priorities. What we're saying is let's get a tool to tell us what our priorities are.

Tom
1:32:28
I don't need to do anything more than, you know, ingesting context so we give this AI some chance of figuring out what the fuck we've been talking about. No, but what always happens is whenever you set about these tasks you find maybe three to six different approaches that could have worked We believe that those cover the whole space, like a 100% solution will fit somewhere within that range of prototypes, but we don't take it home. We need to take this one home. What does home look like?

Tom
1:33:24
Home means to 100%. Home means to operational. but now that you're just replacing one word for another there, specifically, what would a tool look like? Tell me what 80% means then. 80% would be, we've got a screen up with an old one there with our previous day's conversation,

Tom
1:33:49
access to our conversations over the last month, month that you and I can talk to, ask questions of, take that transcript into the next day's version of the same tool. That would be 80%. What? Because what I can see happening there is that if I may offer a partial solution, is

3
1:34:19
that at the end of each day's transcript, there would be some kind of a formatted output,

Tom
1:34:26
which would be like an update to some tasks, in addition to some tasks, the creation of some tasks. Some definitions were changed, something, this and that, right? And so there's almost like this summary of each day's transcription that when you look at them all together, they first of all fit in the context window,

Tom
1:35:06
they first of all fit the analysis pattern of the drill down, I think you were calling it. And then they can output a current thing like, you know, what is person X working on, those kinds of things, right? And I believe that you will find there is a small countable number of things to track, like how many How many reasoning domains do we have? Which one is the most important?

Tom
1:35:43
Which ones depend on other ones? Where are we at with each one, right? And you can start to build up a structure of a format that takes shape over time. You will start to understand what types of questions do you want from this commander? How do you want to engage with it? right like there's a lot of things i can see

Tom
1:36:07
taking form here what tell me then what would be more important than this thing uh... this is uh... It's actually the thing that's at the top of my stack at the moment anyway. Why were you adamant about having a crack at dealing with an external use case without this tool?

Tom
1:36:38
Because our conversation, let's say we had this tool, let's just assume that. Yeah. Our conversation is still quite high level. Now, as a kind of practical engineering type, it's like, well, we might be wrong.

Scott
1:37:02
Let's try it.

Tom
1:37:04
There's no use case that I can imagine that anybody could bring us that would test out these theories. would suggest if that is your concern and it is a valid one is to try and use o1 to generate a set of proving what do they call them proof runs you want all the problems that people are coming to us with our classical problems we have it We have, I don't feel very grateful, but I know I should be grateful. We have inference level problems.

Tom
1:37:46
We have problems about what happens when the AI is one more year smarter. We have those kinds of problems. And so coming up with proofs to know whether our assumptions are correct, we can only get that from synthetic scenarios from O1.

Scott
1:38:08
Ah, okay. No, that works. Right. By testing it practically, synthetic also works.

Tom
1:38:13
Yeah, right. So that's different. At no use case that someone comes to you is going to solve that. The CRM provides a very good practical test case for the code itself. That almost feels done now. I'm like, that's actually, I think that's pretty straightforward now. But that's different because the level that we're talking about is like existential level.

Tom
1:38:45
So, the only difference between that and what was implying in terms of testing it is the test. Now where the data comes from, it's a pretty big difference, real world or synthetic. It's not different between real world and synthetic. Yes, there is. There's a huge difference.

Tom
1:39:01
We can't, I guarantee you that you couldn't get real world data that match the synthetic type that we need in this case because the whole thing is theoretical.

Scott
1:39:10
Okay. I disagree on that one because you could easily

Tom
1:39:15
and I don't, I just see no value in that argument. But the point being you're saying, okay, so let's test it. What I'm saying is let's test it against something concrete to see how we use this. Now we've got a number of things we could test. this AI overlord is going to be telling us.

Tom
1:39:38
We could test it concretely against the construction of the AI overlord. We can test it concretely against the construction of the Dreamcatcher. We could test it concretely against the construction of a CRAM using the Dreamcatcher. All these things. Now, you're saying, hey, let's just synthesize each of these tests. I'm saying we need to test.

Scott
1:40:07
Because otherwise, it's just a conversation between you and I.

Tom
1:40:12
You lost me, buddy.

3
1:40:15
Okay, well, let's just run with it.

Tom
1:40:16
All I was asking was, what's more important? Why did you want a real-world test? And you're saying, well, we need to test the thing. And I'm like, well, the thing's not even defined really. So we've just picked one component of this, which is the overlord.

Tom
1:40:37
And I'm like, well, if we start with just that one overlord part, I can immediately see that our lives will get better. Okay, so what I'm arguing is we need to test it against something. We can't just theorise our way out of this.

Scott
1:40:55
In this case, the AI overlord you just described.

Tom
1:40:59
We have the test, dude. I will know when that thing works or not. Won't you?

3
1:41:04
You don't have... We need to test it, man.

Tom
1:41:08
Won't you? Won't you know when this thing's working or not? Because I damn well will.

6
1:41:17
Well... Who else are you making it for?

5
1:41:20
If not for us.

Tom
1:41:22
Ah...

3
1:41:24
Alright, well we'll see if it works for you,

Scott
1:41:26
um, but it doesn't work for me, and we're going to tell. What we're going to be talking about. Won't you know? You'll be like, oh it doesn't work for this and that,

Tom
1:41:32
and then we loop it again. And then we both try and come up with fixes for it. All I'm saying is, you can go so far in theory.

Scott
1:41:40
Yeah, but what's the point of that?

Tom
1:41:42
It's like saying like E equals MC squared. Of course. Of course.

5
1:41:46
Yes, but you're saying, let's do two tests.

Tom
1:41:49
Dude, dude, I just want the Overlord. I will know when it works. You will know when it works. Let's keep going until it works. Okay, let's roll with that because it's only a day, right? Well, it could be a month actually. Well, it's only a period of time, probably quite a

Tom
1:42:11
short one. So we'll get, okay, so we'll get, we'll get our 80% version tomorrow and then we'll iterate on it again, maybe next time it's 85. Yeah, yeah, right.

3
1:42:26
Yeah. Let's do that.

Scott
1:42:30
I've already, I'll not go back through. The structure of it, the structure of it is pretty easy.

Tom
1:42:39
It will, however, unfortunately because Cursa doesn't allow currently Pro in there. Yeah.

Scott
1:42:49
It will be a cut and paste backwards and forwards.

Tom
1:42:51
Yeah well mate that's what I have to do. I've been cutting and pasting like crazy. Hey man. It's just like. Yeah sorry man. All due respect, don't talk to me about cutting and pasting. I've been bleeding from my eyes with cutting and pasting. Honestly. What do you want me to do? This time it's not my fault. Like we don't have the API for the AI.

Tom
1:43:16
It's just copy paste. I'm sorry.

Scott
1:43:18
Yeah, yeah. So, okay. Well, let's talk about that then.

Tom
1:43:22
Tell me what you think I'm doing then. I think you are working on using the processes that are leveraged by Pro to get a NAP to ask another NAP a question and get an answer back. Yep, yep, yep, that's what I'm doing. Before that though, I am doing chat downloading. Is that correct? Why are you doing that?

Tom
1:43:59
So that we can store these chats and process these chats because they're not currently um...

Scott
1:44:04
Ah, that's not, that's not fundamental.

Tom
1:44:07
Isn't it though? Like, because how do I, I want to reason about the chats. How do I get them down so that I can actually use them. I also process that. I usually have a cup of tea and a couple of digestive biscuits while I put it through a YouTube extractor because it can't be ours. No, it's the chats the chats the on one pro checks

3
1:44:30
Okay, yeah, no fair enough there's no

Tom
1:44:32
Practical workaround for that one. That was a very useful tool. Okay, so I'll make that and I think that's about It's probably about two hours as opposed to 30 minutes just because it accidentally ends up triggering moderation from O1 Pro. That's the only reason. Because it thinks I'm trying to figure out how it does its reasoning in the background. Which is, I mean, that's fine. If I want to protect that, perfectly fine. But I'm not trying to get at that. It's just that the words sound the same. Because it's like reasoning. Just a word of caution on Sora because I was getting

Tom
1:45:20
a bit grumpy because I wanted to have a wee play of Sora and it's not in the UK. So I thought I'll just throw up a VPN and then got a quite a strongly worded message saying don't do this we'll ban you so be careful don't get banned because we've put up quite a lot of you know kudos yeah that's that's that's the issue though um chat gpt's design is not sticky because if i get banned so long as i've got an export of my data which i would have if i built the chat tool as well i do periodically export my data i can just start a new account and then I'm back where I started. That's

Tom
1:46:05
why their platform doesn't have stickiness, it has attraction.

3
1:46:08
Yeah.

Tom
1:46:09
Right? That's different. But it is a pin in the arse though, isn't it?

4
1:46:13
Oh, yeah, yeah.

Tom
1:46:14
I backed well off the SORA thing saying, I was just looking for a... Yeah, yeah. So I just want to put it out there that you need to watch out because they are product people at heart from living in the valley of San Francisco and they will be trying to make this sticky as well as attractive. They've gone with attractive first, that's what they all do. Sticky is coming, so we just got to be able to detach at the right time. Okay, so tomorrow what I'm doing is I'm making the chat our own O1Pro conversations.

Tom
1:46:46
Yep. And then I'm working on the components required to make one NAP call another NAP in natural language. Yes. And I'm working on the reasoning that allow us with cut and paste to have a screen up here for this next conversation where we're talking about something and that something

Tom
1:47:10
And that's something I think should be this conversation and say the conversations we've had over the last few weeks. I don't get that. I thought we were talking about you were making the overlord. Yeah, that's overlord. So we're talking to an overlord who's been prompted to keep us on task, to remind us. Oh, right, right, right, right. I was imagining our next call would go not me talking to the Overlord, although that's, you know,

Tom
1:47:46
if you have something ready, we can definitely do that. But I was more imagining it would be us talking about the structure of the Overlord, how you went about it, what some of the issues are, and all that, as the next range of iteration.

Scott
1:48:03
Very possibly.

Tom
1:48:05
Either way, we can just meet up, we'll meet up next call and we'll go over the Overlord in some capacity.

3
1:48:11
Yeah.

Tom
1:48:12
Right, and for me, I'll turn up and you'll have the ability to archive your chats or ingest your O1 Pro chats and partially I'll have something to say about the NAPs because I now have to reason through the NAPCS design and it would be good to chat with you about that because I find it much easier to discuss and expand on a specification when I'm talking to you or someone else

Tom
1:48:44
suitably intelligent to sort of question. Intelligent but naive is actually a really good conversation partner because you have to explain it to them and they ask Natural questions in natural order whereas as I've worked with this for so long. I no longer am natural I Yeah Yeah, and so that's actually a really useful way to extract out better information So I can have that kind of chat with you, and then I can use that to push forward to my own efforts Yeah, cool. Let's do that. Okay, right. So I've got a plan. We got a plan. What is it today?

Tom
1:49:22
What it's Saturday for me. Okay for you. It should be said there Okay, I think is sorry. I'm losing track a bit Okay, I think is sorry. I'm losing track a bit I'll stop the recording, huh? Yeah




Transcribed with Cockatoo
