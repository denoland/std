Scott:

By tomorrow, I'll catch up at the moment.

Tom:

It's a bold claim. Tomorrow's a long way away in the distance. The amount Yeah. Is is a lot now.

Scott:

Alright. Okay.

Tom:

So the fun stuff. We got OpenAI doing an annoying thing where they are calling it the twelve days of Christmas.

Scott:

Oh, I guess I saw that.

Tom:

Yeah. Yeah. Okay. So they released yeah. Small

Scott:

bars. Small.

Tom:

Blah blah blah. But they released o one. O one is here now. It is the full model. It is polite.

It is smart. It is everything you want in a companion.

Scott:

Have you had a chance to drill it in the any difference between o one and o one preview?

Tom:

Yeah. It's not autistic. O one preview blathers on for ages, and o one preview, like, thinks for twelve seconds if you just say hello like a startled nerd. But, o one, you just go hello, and it's nice and quick. Okay.

And now it's it's also it's also polished. Right? Like, how does the scramjet work?

Scott:

Thinking Oh, it's

Tom:

And it's just the

Scott:

It's not showing you. It's not processing trust.

Tom:

Oh, it is, but it's just like

Scott:

It's just too fast. I mean

Tom:

Well, there's not a lot to think about because it's a straightforward question. That's one of the features that they said that they fixed first.

Scott:

Well, ask us something more complex. How does a a scramjet model that requires a turbine jet relate to business? Let's give it something a bit more meaty. Let's see what it thinks. Sorry.

I'm burning tokens here, but well, that's what I do. I'm the

Tom:

There used to be a term

Scott:

for a Navy called a WAFU, which stood for one or two things, either weapons and fuel user or wet and fucking useless. I think I'm I'm a similar there's a similar thing for token user. Damn. That's fast. Oh god.

Okay. Nice one, guys.

Tom:

But so it's like it's it's etiquette is good. Like, the length of reply is nice. Like, it's

Scott:

Yeah. Yeah. It's not giving you, like, a thousand words for No. You know, twelve words.

Tom:

And it's it's worded nicely. You know? Like, it's structured well. The also, the speed is good because it what it does is it doesn't do, like, excessive numbers of steps. It only does, like, sort of the minimum.

But one of the other things they released, in this twelve days of Christmas is is reinforcement learning. So it's not

Scott:

Oh, yeah. I'm not

Tom:

It's not here. Here. No. No. No.

It's different, though. It's different. It's way good. You should watch it. It seems to be what happens is it comes out on YouTube, and then you gotta kinda watch the YouTube and piece it together.

But what they've done is they allow you to do fine tuning where they're not fine tuning the response of the model. They're they're fine tuning the reasoning pathways that the model takes. So with with a very small number of corrections, the performance gain is appears to be quite significant because Okay. Alright. Yeah.

Because it's it's steering the reasoning and so we should be able to

Scott:

agentic out who goes agentic out the window?

Tom:

No. Agentic's always valuable. We know that.

Scott:

Yeah. I know. That. I'm a agentic skeptic at the moment. I'm willing to be proof wrong, but I think AgenTic's gonna go out the window.

I'm clear with this and try

Tom:

and reinforce the issue. We should come up with a good

Scott:

Thank you.

Tom:

Sort of reasoned argument as to why we both think that it's different things.

Scott:

Well, also because I'm not entirely sure I'm right.

Tom:

It's hard to know who is right around here anymore. But so the point of GPT Pro though is that so these these benchmarks, I think, are unfairly negative to o one Pro where they show an improvement. But what appears to be happening is that they are allowing o one pro to grind like buggery through the reasoning. So what would typically reason for, like, you know, ten seconds, fifteen seconds, like, that'll be a big job for o one. This just keeps going.

And the quality

Scott:

just, I'm sorry. Go ahead. Sorry.

Tom:

The quality of the answer, like, it's so it's such a long process that they gave it a progress bar. Yeah. And that's what the two hundred bucks is for because, yeah, the the quality of output appears to be superior in these benchmarks that aren't really testing. Like, you can't benchmark a really hard problem. I don't think.

Particularly not a novel one. And you I mean, what I'm

Scott:

Yeah. I was

Tom:

What I'm trying to say is, like like, the the problem when you have a really hard problem when you have a really hard problem, the benchmarks don't mean squat really. Because it's like it's a hard problem, and you would be the only one that really

Scott:

Right snake. It's like, yeah. Blah. There's no benchmark that you couldn't use, which mean nothing, actually.

Tom:

Well, no, it's not even that. It's not even that. It's just even if you had a benchmark that was made of really hard questions, that doesn't seem to mean much when it's your hard question because it's so unique that you just want, you know, any improvement which gives it a good

Scott:

Yeah. Agree. Agree.

Tom:

Yeah. But So that's what Pro does. And Pro is two hundred US a month, and I hope that it comes to the API. But it will be expensive either way. I I think we should just get one just to play with it because it So are the

Scott:

is the token burn same cost? So two hundred

Tom:

Yeah. Burn gets

Scott:

you access and then the token burn is the same.

Tom:

No. Two hundred bucks in chat GBC pro gives you unlimited usage of o one and unlimited usage of pro mode as far as I can tell.

Scott:

Really? Which

Tom:

means that you could clock some significant token on prem.

Scott:

I've been I've been hammering tokens so yay.

Tom:

It might actually be cheaper than our, API cost actually.

Scott:

Yeah. It might well be.

Tom:

Because o one is a pricey pricey beast. Actually, that would be cheaper. It would be, because I We've also been holding off. Right? Like, I love o one, but I have to stop because I'm like, oh, I I shouldn't I shouldn't.

Scott:

I know. And I've been going back to four o one Mini and Gemini and then pulling in threads. Yeah. And then o one sits there for thirty seconds going, let me think about that. And you're like, oh, is this costing?

Tom:

Yeah.

Scott:

I think it's a it's a good thing to do. Yeah. Is there any so you've already talked about qualitative improvement. Sorry. My brain same.

I'm still feeling quite awesome. My brain is not working. That's okay, buddy. But so are other than the course of nature nature of its responses, you know, is there any improvement that you've seen in terms of the quality of the reasoning?

Tom:

I haven't tried I haven't tried o one pro mode, and I haven't seen in detail any demos that it's done other than these benchmarks, so I can't speak to that. But given the massive gain that reasoning gives you anyway, I would Yeah.

Scott:

I

Tom:

would I would say absolutely for sure, you know, that that the extra reasoning makes it better. But I just sort of want a glimpse at the notion of this reinforcement learning thing. That means that if let's assume o one pro comes to the API, which means that you can direct it to keep burning, burning, reasoning, reasoning, reasoning for, like, long periods of time, and you can provide training that steers it between how it should reason, then that means that you can control some super deep deep reasoning things going on.

Scott:

Can I can I explain why roughly kind of on the, you know, the the anti side of that that comment? So with suitable prompting, you can filter without having to provide any examples of what you want. Now if we're working in a world where, you know, what we're trying to do, we don't know what people are gonna come up with. What's the training set? What what we're gonna be doing?

Tom:

It's a stunk. We don't we don't we don't need to the the the benefit I see is that stucks the issue with stucks and then making a fine tuned model, like, like, why we've actually done no work on fine training on our side of things here is because

Scott:

I I played I played around with it. Yeah. But Thank god.

Tom:

None it's never come up in our designs or our plans. And the the the reason that I had for that was that a stack is a fairly one off thing. And to have meaningful, fine tuning data, you need quite a lot of things. Now we could synthesize them. Sure.

But the fine tuning was really about the shape of the model responses as opposed to the correctness of it, which is really what we were

Scott:

What do you think you've read more about this, obviously, than I have. What do you think the arc of, these guys' dev is going? Is it towards fine tuning, or is it towards just one model?

Tom:

It's towards fine tuning. It's towards fine tuning because, it's to the overall universal problem is context. Doesn't matter how smart your model is, it's worthless without good context. Yeah. And Yeah.

To give it context in a way that shapes how it thinks is really hard in traditional fine tuning. But with reinforcement learning, by being able to fine tune how it reasons, not not not in the conventional way where you're like, I need you to end every sentence with sir. Not it's not that. It's just saying that the output is how you got there.

Scott:

Yeah. But the the problem the reason of, I'm generally on the, not fine tuning camp, is that, take a practical situation like the CRM. The only thing we can generate either is to troll through all previous transactions or generate synthetic data, but it's still going to be a relatively small dataset. So how much influence that has over the weight of the full dataset they've got, I'm dubious about. I'm willing

Tom:

to No. No. Look. It's it's not about what it's like for them. Right?

It's it's it's how easily we can steer the model. Like, reinforcement learning is perfect for stocks because you only need one or two stocks, and it can be solved with a reinforcement learning.

Scott:

Oh, so what sorry. Sorry. I'm I'm not You're

Tom:

really not so good.

Scott:

I'm not doing my silence thing. Sorry.

Tom:

You're not burning the reasoning tokens? No. No. No. What you're saying is You could burn some.

I I'm ready for you this time. I came up with some hold music.

Scott:

Switch out of mine. Oh, excellent. Excellent. Yeah. So the this is probably not helpful because I'm gonna start singing at any point, and nobody nobody wants to sing it.

That's one of my favorites. Yeah. So we've got the possibility that the information, given a large enough stuck network, we've got a a a training network there. I think what you're saying is why not use the evidence that this is actually stuck in order to solve it by fine tuning the model and saying solve this with all this information that we've gained. Is is that what you're saying?

Tom:

No. I'm saying that if in the worst case scenario, we're stuck as a one off thing and we can't find any other any extra examples that we can use as a traditional fine tuning set. Being able to use reinforcement learning to improve the correct reasoning pathways, ones that gave a better outcome than other ones, allows for a more general solution to the problem because now because you've corrected the way that the model or you've steered or tuned the way the model thinks about things, and that is what has been giving you the benefit of an improved outcome that has a far, like, remarkably higher chance of a new similar problem reaching the correct output because the thinking was right as opposed to having the tuning have shifted it. Like, reasoning is remarkably more steerable than fine tuning. Like, maybe a million billion times more steerable than fine tuning.

Scott:

Yes. Right. That's really not helping. Although I do like it. I think it's funny.

But the I'm I'm not gonna die and ditch over this particular point because I don't think we know yet. But fine tuning requires some a priori knowledge And on a stuck, you don't actually know where you want to go yet because it's a stuck.

Tom:

Oh, I meant sorry. I'm I meant not a stuck. I meant a sole stuck that we wanted to get a better performance from the model with.

Scott:

Oh, yeah. Yeah. Oh, yeah.

Tom:

Sorry. Sorry about that. No.

Scott:

No. No. No. No. That that works.

Absolutely. Yeah. Okay. Yeah.

Tom:

Alright. So so that was pro mode. O one is out. O one is beautiful. Reinforcement learning the future.

That leads us to a more deeper problem that I wanna talk about because because

Scott:

Because you do. In Oh, there's the I can vector the thing.

Tom:

The the correct the correct way to use AI is to tell us what to do. Because we are bad at that and the best of us become billionaires or some other kind of revered person.

Scott:

But He's cracking the cord. Right? So bear with me on this analogy, but Elon Musk Hang on.

Tom:

Why don't you let me finish? Because I was trying to say something that took some time, and now you've jumped off on an analogy about something that you almost certainly don't get because I didn't even finish saying it.

Scott:

Excellent point, my mate. After you, sir.

Tom:

We need to this reasoning topic is a beautiful vein for us to be going down because it produces long lasting knowledge that no matter how good the models get, the reasoning is going to stand in principle. It may be, improved and all that kind of stuff. And so the question becomes how high up can we make the reasoning? You know, like, can we make it run our whole lives? Can we make it run our domestic relationships?

Can we make it run, you know, all those things. And so at the very least, the next level up from where we're talking about using the reasoning is to say, what is the dream catcher about? Where it's shortcomings, etcetera, etcetera. What are the logical steps that we should undertake? Agreed.

And

Scott:

Yeah. Yes. Sorry. Yeah. Okay.

Tom:

Also also have still still going still going I need a I need a talking stick. This one is currently it's it's hard for me to say it because I don't really know how it goes. But the reasoning, clearly, that's power. But we're not the the the skill seems to be to ask the right questions

Scott:

Yeah.

Tom:

And to be able to ask the model, what should we do? How should we do it in the right way so that when new information comes, the response is good. Now I I get this instinct that we can make GPT pro grind for the cumulative sum of, you know, hours and hours and hours to come up with finally thought out reasoning as to what the dream catcher is about, how we should go about doing it, how does it relate to its competitors.

Scott:

Yeah. We get the shit. Sorry. Go on. How do you mind?

Sorry.

Tom:

No. That's that's that's that's it.

Scott:

Yeah. Yeah. I think I know where you're going. And before I almost, you know, died, whatever, with some man flu. What I was doing was using, reasoning against the definitions of dream catcher in a cyclic manner.

Now that cyclic manner generates a lot of tokens and a lot of reasoning. Now one of the things that I fell afoul of partly as a cursor is great. I love that too, but it falls down on the targets. And so there's still quite a cognitive overload in terms of I'm actually talking about this now and so on. But Right.

That being said, you know, c'est la vie. But cycling rounds, talking about at a metal level of a process is hugely productive because it tightens it up. Now remember I don't know maybe five calls ago I was talking about the definitions in some kind of bell curve where

Tom:

Yep.

Scott:

The definitions gets larger and then they get smaller and tighter. Yep. That's what that cyclic curve is for.

Tom:

Yeah. Yeah. Yeah. So so being able to use o one with these, like, unlimited credits and unlimited thinking time to basically make those definitions be as small as they possibly can. And the the thing is, the examples I'm seeing is people trying to use this to help them do things like genetic sequencing, you know, weather prediction.

And I'm like, that's not us. We have a hard problem, but it's not even that hard. So surely, this new tool could help us.

Scott:

It's similar to genetic sequencing, although those guys have got harder. Yeah.

Tom:

Yeah. So the that's sort of one pillar of the the argument or the shift that I'm trying to push on us here. The second pillar is that with internal reasoning, it might be that a lot of the things that we wanna figure out can be done internally. As in this model has read every text ever written more than

Scott:

us. Oh, I see. Sorry. I see where you're going on that one. Yes.

Sorry. Damn it. I was I was trying to I was trying to get better over the last few weeks, and now I'm gone back. Yes, so Deborah, hang on,

Tom:

you just like interrupted, said sorry, said you're gonna be better, and then carried on with the interruption. It's like

Scott:

I know, buddy. I knew you were going with this.

Tom:

Do you? Tell tell me, and I'll I

Scott:

think so.

Tom:

If you get it wrong if you get it wrong, what should the punishment be?

Scott:

A good whipping around the back of the hut.

Tom:

A good A good a bit. Okay. Tell me what I was gonna say.

Scott:

I think you're you're alluding to the fact that the fine tuning part of it with reasoning actually amounts to effectively it could be you can call it an app. It's it's a capability that others don't have because you don't others don't have access to the fine tuning data that you fine tuned this large model with.

Tom:

No. There wasn't at all. What I mean, it's not it's not a bad idea. It's an interesting thing, but that's not what I was trying to say.

Scott:

Okay. Alright. Well, I'll tune myself out when I put myself later on. Okay. What were you trying to say?

Tom:

Inside of o one is all human knowledge. Yeah. And we keep trying to feed it information that we created, like transcripts and whatever, and ask it to reconcile things. But there's a large range of, tasks that it it might be able to self reason about because it already knows all the stuff. Like, you take this stuff we've been reviewing with a zero, the blockchain.

Scott:

Mhmm.

Tom:

O one knows about a zero. It knows more about a a zero than us. And so if we were

Scott:

to a bit. Yeah.

Tom:

Yeah. If we were to tell it to grind through a making a comparison system between all those things, and then we sort of the issue in cursor or even in o one plane is that you can't it's it's painful to refeed its stuff back in.

Scott:

Mhmm.

Tom:

But in o one pro, it sort of just keeps on going. And you know what I mean? So it might Okay.

Scott:

I'm sorry.

Tom:

We we might be able to get these comparative, documents out with just so long as we got good at asking really good questions at the start, this thing might be able to generate a lot of what we're looking for provided you give it questions that it can fight against itself a lot.

Scott:

So I've not used pro yet, obviously. But the thing with preview was it lost the track. The longer the conversation goes on, the more it loses your train of thought. Now I don't know whether pros

Tom:

Yeah. Yeah. But but this is it seems that within its own reasoning, it doesn't seem to suffer that. And so if you can make its reasoning go long, that's different to making the check go long.

Scott:

Oh, I see. So you say, right here is all previous reasoning around it.

Tom:

Think once. Yeah. Like, basically, slam up the and slam the tokens up to, like, a hundred and twenty eight k. So you pack in as much context as you can.

Scott:

It's not bad strategy, actually.

Tom:

And then just give it the right kind of question that it would just self it could self check for a lot of the work based on its own internal understanding of the world.

Scott:

You wouldn't even have to get it right first time, the question, because with preview, you're getting half an hour into it and you're saying, dude, I told you about this. I'm gonna have to tell you about it again. And I'm like, oh, thank you for reminding me.

Tom:

Yeah. Yeah. Yeah. Yeah. But the and the thing is that in the way the models work is that all of the reasoning is dumped in the subsequent message.

So it's only dealing with its own final output. It's not dealing with its Right. It's the reasoning doesn't get picked up. But with o one pro, it stays in reasoning mode for a long time.

Scott:

I'd love to know what long meant there, but I get your they call it this idea there. And if it does that, then that would be really cool. By which so in the cyclic reasoning around gene catcher, the kind of meta reasoning about gene catcher I've been doing, I've often had to compile down and then start a new thread because it was just forgetting stuff. Yep. Yep.

Yeah. It was often one, which is a pain in the arse and not very automatable because it required a human brain to say, woah. You're, you know, you're drifting. And what you're saying possibly is pro does not drift if we give

Tom:

it substantive information. Yeah. But the problem the problem becomes how do we I mean, obviously, we gotta play with this thing to understand if it does what we want. But how do we know like, how do we know that our definitions are good? You know?

Like, I read our definitions, and I'm like, that sounds great.

Scott:

That's that's exactly the point I've been working on. It's like I've been going down the three c's and going backwards and forwards and backwards and forwards and getting it tighter and tighter, I think. But actually reading through them, you can't tell.

Tom:

You can't tell. That's the problem. Yeah. And so there's a point where it's, like, it's good enough to use, and it's only from the use that you realize that something's missing.

Scott:

Right. Which is where renders came from,

Tom:

which is the, you know,

Scott:

you've got these definitions, render a model as, like, oh, no. That's not right. Well, let's in answer to your initial question, let's use this this new tool and find out how good it actually is. I've got my own kind of benchmarks, not in time terms of time or tokens, but in terms of, qualitative output. So let's run them.

Okay. I think it's interesting also because anyone with two hundred bucks now has access to this. So this the race is

Tom:

Yeah. So then I'm like, well, should we be trying to reason through all these different things? Right? The security's guardian.

Scott:

Well, that should be straightforward enough. Right? Because we've got our our logic layer, which is the definitions. We've yet to do evals, which bothers me. But

Tom:

Yeah. I know that kind of fell by the wayside. Hey. But it's like, man.

Scott:

I'm not yeah. That still bothers me because I don't think that's something that a new model is actually gonna do for us. I still think that's a piece of work to be done. However, we do have our base logical there, which I think is a a really strong way of thinking about these things. Now we should be able to now okay.

I'm I'm jumping around a bit. But remember I was saying just before I went off sick I took a transcript. Literally just, you know, someone talking about a system Yep. And then put it in Yep. And then there was a bunch of definitions.

Now our previous talk was you're saying, okay. So take those definitions and then transcribe those into JSON, and now we've got some meat that we can actually deal with. That process is pretty sound, actually. I'd be confident to walk into, you know, our friendly brokers room and do that live.

Tom:

Yeah. But what's the what's and now we're getting to the heart of the question here. What's that mean if everyone can do it? Like what are we offering? Like all that does is help us know what we should do, but what it doesn't that's not a product.

Right?

Scott:

No. Absolutely. And

Tom:

That's like a strategy. That's like saying being the guy that invented Gantt charts back in Yeah. Whenever. And you're like, I got this technique, and you're like,

Scott:

so Order order friend, yeah, invented, you know, a spreadsheet. Yeah. The the I still think that our gold is the idea of the stock. Still think that's the thing. Well,

Tom:

I think that I I I think that I agree with you, but what I think is that that you've stripped off some core components. When you say stuck loop, I think you also mean ambient attribution.

Scott:

Oh, yes. Stuck loop, requires ambient attribution and

Tom:

Does it?

Scott:

The record of contribution in order to incentivize. Yeah. That's That's that's

Tom:

no. That's our version of it.

Scott:

There might be another one, but who wants to, you know, stand up and give a better one? Crack on. But, essentially, what we're saying is if you rely incentives

Tom:

Yeah. No. You don't need to tell me about the DreamCatcher, sir. Thank

Scott:

you. So Okay. Sorry. I'm so sorry.

Tom:

You're sick, buddy. Don't worry about it. So the the but the question remains. Right? Like, we got we got reasoning.

That's great. That's not a product. That's more like a management technique. We've got, o one pro that has nothing to do with us. It just fell out of the dear blue sky, demanded an eye watering price for a SaaS subscription, and then was like, you know, so what?

I'm the smartest machine on the planet. Pay up. I'm like, well, fine. Okay. Wow.

Scott:

Yeah. I think you're over the indexing on product.

Tom:

No. I'm not. No. No. What's the product then?

Scott:

Well, no. I think you're over to indexing underneath for a product. What we got is capability. Right. Remember when Now now

Tom:

now you're making me think you really are delirious. The thing I'm trying to get at is is is value. Where are we delivering value?

Scott:

No. Value. Value is not a product.

Tom:

Fine. Carry on then. Tell me where the value is. Capability. That's just a fucking word.

That's like saying the value is on earth somewhere. I'm like, not helpful.

Scott:

Capability of two cars. One goes at one point zero

Tom:

Don't care about a meter four. Where is

Scott:

it for us? One. If we are faster than the next guy, we're better. What? We're faster than the next guy.

We're better than

Tom:

the next guy. Yeah. Doing what, though? Like, if if like, cars don't sell because, like, all it does is go faster. Like, no one's like it's like, so you made the fastest car on earth.

Congratulations. Now Never. How will you pay the rent?

Scott:

Businesses do, and it takes a a real small fraction of an improvement in order to

Tom:

Yeah. I know that. How does that how do we implement that with what we have? Because what we have has deviated significantly or decoupled, detached, splintered from implementation and has become simply turning up with a tool that someone else made. How is that valuable?

Scott:

The value is in how you screw these chunks together. Right. Is there any tool Do you

Tom:

wanna take you sound like you should be burning tokens with that thought right now. Do you wanna take some?

Scott:

Yeah. Yeah.

Tom:

And actually, I'm gonna

Scott:

make a

Tom:

Scottish you can

Scott:

take take a moment to, apologize. I'm still feeling like

Tom:

That's okay, buddy. Do you wanna stop?

Scott:

Yeah. No. I don't because this is really interesting. But Okay. Well, you

Tom:

just sing out if you wanna stop. I I I know he's sick, and and I feel

Scott:

I'm asking I'm I'm asking I'm asking for your patience. Okay. Me being a bit belligerent or wrong or, you know, incoherent.

Tom:

That's right. I feel guilty that I'm arguing with a sick person who's just had a delirious disease. But I'm like, well, you know, we gotta get this thing done. Yeah. No.

No. I've got a sense of urgency as well. Do you want the back pipes back? Or Well, put some run rig. Some what?

Sorry?

Scott:

Run rig.

Tom:

What the hell is that?

Scott:

Oh, actually, hot No. You don't. No.

Tom:

No. No. You otherwise, people will just call up the hotline to be on hold. So why don't we just you just do do do your thinking, and then

Scott:

Alright. Okay. So what my thinking is you're not asking what's our what's our competitive advantage. Is this really what you're doing?

Tom:

No. No. No. Not really. I'm saying that the things of value that we think can be of benefit to innovation or business as a whole Mhmm.

Have become decoupled from a cohesive platform that we present and have become available in equivalent components.

Scott:

Okay. Can you

Tom:

To pick a specific example

Scott:

Oh, yeah. Yeah. Go ahead.

Tom:

Yeah. Yeah. A specific example is this reasoning technique that we've got. If we were to capture it into a platform that we made, is that valuable? Yes.

I think that it is useful to use for ourselves.

Scott:

And it's easily copied as well.

Tom:

Easily copied, and we'd kinda give it away. And Mhmm. People wanna pay us for the tokens, fine. We'll we'll charge for tokens. But that's that's a side effect.

Again, the blockchain piece. Well, if the decentralization matters, we've got some other people who are likely to have done it first. Mhmm. Better. And so, you know, so what there?

Like, should we just glue into them? Now we have this reasoning platform or sorry. Reasoning SaaS app that runs on someone else's blockchain that does the stuck loop.

Scott:

Okay. I guess you're right. Product is the right name for it because what we want to do is to produce something that others consume. Yeah. And then everything that's gonna go by.

Yeah. And Yeah. Yeah. I was talking across the properties because I was talking about the the mode of thinking about AI and so on, which is actually in the fact you can't defend that as part a piece of IP because ideas are not IP. That's right.

So in terms of a defensible product, I guess it follows that we should be clogging in the likes of a o and o one as quickly as possible and using that as a reasoning for improving a product, that product being the dream catcher. Now we've also got the decentralized AI layer which, as far as I say, probably someone else is working on right now, and I wouldn't be surprised if it just popped in a blue sky.

Tom:

Yeah. Yeah. And so I I don't know how this helps us, but, you know, we we need to know about those things, and we need to have an opinion on them whether or not they're a suitable alternative for us to use versus our own our own structure?

Scott:

Well, what we do have as right now, because we are nobody knows about us. What we do have is a very good, I think, defensible way of reasoning with the bots where we get excellent results. And and so maybe what I should be doing is doing a bit of competitive research around who else is doing it. And maybe someone's done something No.

Tom:

I don't think so. I don't think so. I think what you should be trying to use the reasoning techniques to do the research in a way that it's o one that's actually doing the work. And then we should try and capture that into a platform that makes it so there's zero cut and paste and, you know, you don't need a pro subscription and all that because

Scott:

Yeah. Okay. Yeah. Right. Yeah.

The the first part is straightforward because that's what we've been doing anyways.

Tom:

So that but that capturing that into a product is an entirely separate exercise to building the dream catcher. It appears to be required because it turns out the dream catcher is quite a large thing, and so having this AI assisted reasoning becomes essential. But, you know, it's independent. The NAPS platform, that's a good thing. That's independent.

Scott:

Yeah.

Tom:

The blockchain way of running it is independent.

Scott:

Yeah. And what you were saying other two calls ago about the compilation of reasoning into JSON, I think is a really good idea. I've been thinking about that a lot trying to put calls on it. And I think your reasoning is sound. Your thinking is sound.

Tom:

Yeah. But that that only works at the concrete usage layer. It doesn't work at the strategic layer. Like, what is our strategy for the next year? How will we spend our money and our time?

Scott:

I just I I disagree because the what we've got is core reasoning and compilation down into JSON. And then the whole point of putting

Tom:

it

Scott:

into JSON

Tom:

is that

Scott:

you populate it. You populate it with information, and then you can reason about the information that you populated because it's well structured.

Tom:

Right. Because it's it's baked in a sense. It's it's

Scott:

Right.

Tom:

Yeah. Okay. Alright. Yeah. Okay.

I'll give I'll concede that. So what we're really trying to construct then is a set of software tools and techniques that allow us to answer any business problem faster with higher grade quality with far more base data than than ever before so that we can sort of start to run a lot of things on autopilot, and we can actually remove the anxiety from what we're doing because we're like, well, we can see everything. And Yeah. We know that this o one model o one o one pro or any model doesn't have to be the best reason for it to be the thing we use. It only has to be better than us.

And I think what I'm actually saying is that I can see that in the case of strategic direction and allocation of resources, provided we can ask the right questions, o one can do a better job than I can.

Scott:

This is this leads on to another thing, and, you know, I love you like a brother, and I hesitate to say this. But unless you're working in NL, you are anyone is going to be outstripped by an order or two of magnitude. It doesn't matter what you're doing. So the likes of artifact and so on.

Tom:

Like So even building artifact is so building artifact, but even though I'm leveraging, say, cursor to help me with that, even that is lost.

Scott:

Yeah. Yeah. And we're we're already seeing, you know, these guys, the the old guys, they're saying they started up in February. We were talking about the stuff they're talking about last, you know, November when you're over here. Or was it even last November or was it year before?

I don't know. We were

Tom:

talking about It was not it was one year one year and one month ago.

Scott:

Yeah.

Tom:

The coldest month of my life. Well, they're young and they're enthusiastic and they've got a token. But we need to figure out how do we deliver the payload. The payload is ambient attribution running in the the stuck loop with an ambient attribution cycle. And we need to be very practical about how we deliver that.

And if we can find a well reasoned substitute for artifact, we should take it just because

Scott:

The the our our artifact is, is your partnership. And what I'm just saying is, unless you're, leveraging, LMs, to the max, then we're gonna get outpaced on that part.

Tom:

Okay. So so being brain used

Scott:

to be in the NL

Tom:

world So getting outpaced as

Scott:

quickly as possible.

Tom:

Getting outpaced in code means that to avoid that, we need to take a a gear shift pause, and we need to move to reasoning so that we develop artifact. First of all, we reason about the need for artifact, and does it even need to be commissioned? Mhmm. Or can we pull something else off the shelf?

Scott:

That's an interesting question. But yeah. I think.

Tom:

And then if we decide to commission it, the reasoning needs to be done first before any code is laid down. And that's probably the difference. Right? Because right now, I'm like, I can sort of see how to do it, so I'm trying to do it in code. Yeah.

But what I think you're saying is that I'll get outpaced from versus someone who just took a pause, did the reasoning, used the latest models to grind out all the logic, and then came at it using the models on the way down.

Scott:

Right? And and for now, but this is gonna be stripped from us, pretty soon, is a very, well tuned reasoning model. But it's it's it's not No. Hard for someone else to

Tom:

It's not hard. It's not hard. And you we've gotta just imagine that there's other smart people out there doing the same thing. But Exactly. The thing that we care about delivering is using using these hardcore models to do attribution.

Right? And so Attribution

Scott:

is a nut. That is a difficult nut Yeah. To to crack.

Tom:

And we sort of we've tangled up the stuck loop with attribution because we're like, well, it kinda doesn't work if it's just attribution. The stuck loop is required because you get that network effect.

Scott:

Yeah. Our may maybe we've messed here because stuck loop could be internal.

Tom:

Okay. So distribution Either way either way, it's very obvious that we should apply reasoning techniques to the dream catcher so that we can settle even these discrepancies about the dream catcher. Right? Because we have, I don't know, five different products that we could pursue if we if we so choose. Each one of them can be independently done, managed, profited from all that.

Except the dream catcher doesn't seem like No. Because it's the thing that needs all the pieces. Yeah. No one else is gonna do that one. And so that's the one we should reason out first.

Scott:

Oh, yeah. All these all these other products we could possibly use because we were talking about decentralized AI. We were talking about labs and so on. I don't have confidence that some other smart guy given these tools will just, like, say, hey. Done it.

Oh, by the way, I've

Tom:

just raised a hundred percent. Yeah. Right. Right. Right.

Because we we might be we might reason about it, decide it's good, and then miss someone who just turned up and did it, which would have cost us time that we should have spent on the other thing.

Scott:

Right. So, again, your question of about two months ago is what should we be spending our time on?

Tom:

And it sounds like it's the reasoning tooling and the reason and the reasoning about the dream catcher. Yeah? I think

Scott:

so. I think so. I agree.

Tom:

Okay. I I think there's some several strong reasons to do that. First of all, we need it anyway. Second of all, the pieces that we depend on will be affected by that reasoning because we might realize that we don't need some big chunks that we thought we did, like naps, and we can dump those. But thirdly, as you mentioned, given that time's going by and the rate of tech advancement is the fastest it's been in forever, the things we need might just appear.

And so if we spent time on it, it means we didn't spend time on the dream catcher stuff. And so then we're in a not so much a worse position, but it's just that we didn't get to the dream catcher as quickly as possible.

Scott:

It's almost like there's a rule of thumb, heuristic here that, if it seems obvious to us, it's gonna seem obvious to someone else. And with the tools, someone else is gonna do it. Yep.

Tom:

And Obvious is obvious. Yeah.

Scott:

Right. The stuck loop is not, I I would argue, obvious.

Tom:

The I don't think that matters I don't think that matters whether it's obvious or not. Things things appear out of the blue sky that were not obvious to me all the time. Just because something's not obvious doesn't mean it's not gonna appear.

Scott:

True. Agree. Yeah. The point I was making is, what is what is our fallback, you know, in, like, Game of Thrones? You know?

Tom:

You

Scott:

know, you got a number of layers. You go back to your keep, your final fallback. What's our fallback, Ashish?

Tom:

The fallback is admin attribution admin attribution. That's all there is.

Scott:

Know because that involves a lot of thought. I can't imagine

Tom:

Well, and sacrifice because it sucks to do it. You've gotta completely self fund it. Because when I look at the, the a zero blockchain and its fundraising mechanisms, its incentive mechanisms, you know, they did a token sale. They got private equity on board. You know, there's people looking to cash out.

Scott:

It all gets distorted Yeah. Because you've got a bunch of guys Yeah. Saying, oh, I need money back.

Tom:

Yeah. Yeah. Right?

Scott:

I think

Tom:

that's So

Scott:

thinks that that's a fallback.

Tom:

Right. So that's a problem because where does that put us with the CRM? The CRM kinda just needs to be done, but does it, I guess, is the awkward, ugly question.

Scott:

Oh, to to be honest, man, and I'm not going to smoke up our collective arches here, but we could do the CRM in in two days.

Tom:

You mean as a conventional app?

Scott:

Yeah. Well, no. As in an NLLLM enabled app that has core reasoning.

Tom:

How do you get one? So right. So if I ditch artifact so if I ditch artifact and I just use a Postgres database or a SQL database?

Scott:

We could do it in two. Two I don't know what it is. But, you know, like like

Tom:

Yeah. A number of days.

Scott:

We could we we could do it. The, the stuff that we're thinking about and we've talked about for the the, the last hour or so is how to survive in a world that is accelerating in the use of AI. Now that is not what the CRM guys are care about, but they do have some cash that will they will give to us, which would be nice. And it's it's almost like if we just paused the whole, let's stay at the flame front, which is still I do believe actually we're ahead of the flame front by a modest two or three weeks given evidence. If we pause that and just do the CRM and get some cash, yeah, you go.

I don't know whether that's the right thing

Tom:

to do, though, because the flame front is accelerating. We want to use the stack loop inside the CRM because we don't have any other commercial operation.

Scott:

Yeah. But we've got we've we've got this we got we continually talk about having shitty versions and just having the stuck loop improve the stuck stuck. We've got a Right. Stuck loop.

Tom:

We don't have a shitty stuck loop right now. We've got the reasoning layer on it. Right. What we don't do is a presentation layer of any way that Innoin actually produce it. Okay.

So does any kind of okay. This is fun because it means that we can actually even though we agree that we need to do the reasoning for the dream catcher first, there are because we know so much about it or have used it so much. Yeah. There are for me to There are certain things that no matter what the reasoning output is still have to be done. One of those things is the front end where you can interact with LLMs.

Right? Right. It's the

Scott:

volunteer kind of three box thing.

Tom:

Right. But what's what's becoming the most important button in the whole system is this button, the stuck button.

Scott:

Well, it's most important to us.

Tom:

That's well, that's what I mean. When I say most important, I mean, who the hell else would I mean, sir?

Scott:

Yeah. Yeah. Well, what I was saying is for the customer, what they'd like is a perfect product that never changes and, you know, never gets stuck.

Tom:

And Okay. So let's let's ignore them. But from our point of view, this button is It is

Scott:

the most important.

Tom:

That's the entire thing.

Scott:

We should make it coldy looking.

Tom:

You wanna make that gold blue?

Scott:

Make it glow. Make it glow.

Tom:

Oh, like some glitters and sparkles around it. So instead of fixed responses, like, glitter sparkles.

Scott:

Yeah. Because Yeah.

Tom:

I'd maybe. This is

Scott:

the thing. Let me take a step back and kinda go a bit meta on this. Yep. There what I'm seeing is there's a lot of very, very smart people getting very excited about an amazing tool. It's just dropped into the lap like everyone else, like, they have with us.

And they are going health related and going through brokers and the usual thing to get lots of money in order to do what build a company a HR let's get a dev team yes It's a

Tom:

Can we can we skip yeah. Can we skip forward?

Scott:

Yes. But the point being, the whole point of Dreamcatcher is to just say, well, click the bird to that whole thing.

Tom:

Yeah.

Scott:

And so that is that bird is is our maybe that should be local. That bird is our our only thing that we can really actually, honestly, double down on that nobody else is doing. Everyone else is trying to cash in.

Tom:

Everyone else is trying to make a company, and we're trying to make the next generation of what commercial coordination is. Exactly. Okay. And we're doing it through this button. In a weird way.

Scott:

That button is everything.

Tom:

Okay. So no matter what we do in the reasoning on the dream catcher, this UI still needs to exist. Right? Yes. Okay.

And the and most importantly, this button within this UI needs to exist. Yeah? Yeah. So I'm not wasting any time if I build that bit?

Scott:

Oh, god. No. That is possibly the only bit you should bother about because other bits will be built for you at the speed of, you know

Tom:

I'm not convinced that artifact will be built for me, but I need to figure out what we miss out on. Like, that's where the reasoning comes in. Right?

Scott:

Can I with well, you know the respect I have for you?

Tom:

You don't need to say that, buddy, the fact that we're sitting here.

Scott:

Yeah. I know. I don't have sight into artifact. And you say, artifact is complex and so on. But we're talking about getting reasoning as the core about what needs to happen in order for LLMs to generate stuff.

That stuff could easily be called. It could be haikus. Yeah. It could be anything you want.

Tom:

So you're saying that if we get good at reasoning, which we're not good at reasoning yet, we've just done we've started down the path of reasoning.

Scott:

I I think we're pretty we're I don't know.

Tom:

It doesn't matter anyway. It doesn't

Scott:

It doesn't matter.

Tom:

All the the point I was gonna make is that we're not really matured in our reasoning because it's quite a new thing. We're still we don't have any software built to help us. We're still doing a cut and paste or inside an IDE like Chris said.

Scott:

Right. Right. It's it's something similar to that because you were very kind and felt my pain over, you know, the last year or so with the cut and pasting. Yeah. I'm now feeling your pain in having to actually hit a keystroke in order to build code Yeah.

And maintain the coherence and consistency of that

Tom:

when we've got a core reasoning model, which It's like it's like we need we need some there's some little tools that we need. We need to be able to do reasoning in our own platform so that we can apply tools in a unique way. Right? Right. Okay.

So that sort of leaves us back. Can you

Scott:

indulge me with with analogy? Say, if I wanted an API to go out and see what the weather is in Scotland, I'd want to be able to have the definition of weather in the core reasoning. And I just want that API just to be done. Not a single line of code. Right.

Nothing. Right. Now, I think artifact is is not taking advantage of the interconnectionness of the reasoning model that we've got in order to say, right, okay, so you don't need this, you do need that. Now it takes cycles, of course, but you can get closer and closer and closer much quicker than you can hitting keys.

Tom:

So what he's saying is that the building artifact by hand or even with cursor building artifact without reasoning is gonna take longer than building the reasoning and then using your reasoning to build out effect.

Scott:

Right. It's like trying to build a Morgan, which was built by Artisans, and they were amazing at it versus Ford who wiped out the market.

Tom:

Right. But during the time that Ford was constructing his production line, he looked stupid.

Scott:

He did. Mind you, he was right.

Tom:

Sorry. What's that mean? You mean, like, we might be wrong, and so we'll spend all the time and then be wrong.

Scott:

No. I I I I oh, what's the best way to say this? You're you're an artisan crafting code in a world that everyone else we've got a tool dropped that crafts code, but also gives you contraindications, gives you indications, gives you connections, and allows you to reason at a higher level.

Tom:

So the correct path to take is to do the reasoning on artifact before building artifact.

Scott:

I think so. I don't think artifact falls out of this. Okay.

Tom:

So where does that leave the CRM? Does that does that imply we should do the reasoning about the CRM before we build the CRM?

Scott:

Say it again. Do the reasoning about the CRM before the CRM. Yes.

Tom:

Yep. Okay. Well, then to do the reasoning, we need the reasoning about the reasoning to be able to build the tools for the reasoning.

Scott:

Yes. We do. And we've got cursor at the moment, which is

Tom:

Choice is enough to get us started on reasoning.

Scott:

It's a it's a fifty percent solution. Right? Yeah. And it's, like, a hundred percent better than what we're doing before. It's not the it's not the end product.

It's not the way to do it yet, but it is the best. So using cursor the okay. So let me get more specific. The reason the cursor is why I'm so, you know, a fanboy of cursor is you can point it at the context and say, consider this, consider that, consider this. This is paramount above everything else I've said.

In prompting with just apps and folders and files, you can you can fine tune prompts way more than you can do on, you know, any kind of, like, public portal. You can find tune prompts and saying, this is I just ignore everything else, and then you get exceptional results.

Tom:

But but so so what?

Scott:

The point being, I'm a fanboy of Cursor. Cursor is built for code. I'm using it purely in NL, not

Tom:

a single word. Yeah. But the the the reason the reason why you're doing that is the same issue that we have. Like, we're just acting as a conduit or a presentation of the AIs. The AIs are the real star here.

Kurz is just like people think Kurz is great. It's not. It's the AI that's great.

Scott:

No. No. No. No. I agree.

Kurz is people

Tom:

People think our people think our platform is great, but it's not. It's the AI that's great. And And so what is it that's left behind to do? The so cursor

Scott:

cursor cursor is a fifty percent solution, which depends on LMs in order to just not to be BS code. The thing that makes it great is the fact that I can say, right, consider this, this, and this, ignore everything else. This is paramount, etcetera, and get a good reasoning model out of it, and then I can write that

Tom:

to permanency. Okay. So what we need then is this interface here with the ability to control the context finally. Right? Yeah.

So so really you want this thing with a bunch of files in the background that represent the definitions, and then you wanna be able to reason about it in the prompt. Yeah. Yeah. Fun. Yeah.

Okay. But that

Scott:

I know. I know. Okay. It's that hard. I don't know.

Tom:

Well, that in my mind, that needs a piece of artifact. Yes. And artifact, I'd also argue. Now this might be a stretch too far, but there's

Scott:

a whole bunch of guys out there with this amazing tool. We can create, I think, definitions around artifact and do exactly the same thing with artifact.

Tom:

Well, what you're saying is that it's essential to our thesis that we can like, that's you're saying that everything we ever wanna build can be done by reasoning about it first and then using the reasoning to help instruct the AIs to render the solution. That's what you're saying?

Scott:

Yeah. Yeah. And we got re we got reasoning loop and we've got a huge massive, cognitive tool that will help us.

Tom:

That's just getting better and better. Yep. More and more steerable and tunable.

Scott:

Yep. And then can, like, just spew out code. There you go.

Tom:

And good code too, I might add. Yeah. Okay. So then to to to

Scott:

Probably probably better than code that

Tom:

you'd write. Oh, way better. That's way better. But, I mean, like, I I am already using those models by way of GitHub Copilot or Cursor to do my code. But what's really I'm still in charge of laying the base code, and what you're saying is that I should document it first or describe it, specify it, require it first.

Yes. Use the reasoning process that we have sort of hand artisan crafted that we will soon distill down into a machine.

Scott:

Yeah. Yeah.

Tom:

Take the reasoning, get the bot to make the code, and then just sort of twiddle the code and twiddle the reasoning until the output is correct. Yeah?

Scott:

Yeah. And cycle around. And remember the thing I was saying about AI is necessarily our derivative, whereas human brains are forward looking. Yep.

Tom:

So To get that going

Scott:

I feel I feel like I'm being quite harsh here. I I I

Tom:

I feel like I'm being harsh to you, so let's just call that even.

Scott:

Okay.

Tom:

So thinking thinking thinking.

Scott:

Pause. Yep.

Tom:

In order for you to have the cursor like context control within this interface on screen, there is a portion of artifact that you would need that is the Git file system portion. You would not need any of the execution ability that we had tried to put in there because you don't need to do very much in or anything in parallel to do reasoning. To do reasoning does not currently need large parallel jobs. Right?

Scott:

Yes. Okay. Large parallel jobs is just gonna be wiped out because it just getting faster and faster.

Tom:

That's false. That's false. Three minutes. Sorry. Go ahead.

That's absolutely false.

Scott:

Why is that false?

Tom:

That's false because I can immediately think of algorithms to do reasoning that would require large numbers of parallel jobs to improve the quality of the reasoning and the speed at which it comes back. But all that is true is that the way that we are currently thinking about reasoning does not require, multiple parallel jobs.

Scott:

Yes. My, contra to that, which is actually not so common.

Tom:

I I actually don't care to argue about it. I I don't care about being right, and and I don't think it helps us anyway whether we're right or wrong there.

Scott:

Can can I can I indulge you in it and win and and see it? The indulgence of waiting two minutes for a correct reason is better than having something that is inferior after two seconds right now. Yeah.

Tom:

That's that's irrelevant. No. It is. Anyway It is. Yeah.

Okay. Alright. We need we need the we need the reasoning. Let's just assume that somehow or other it comes out. And I'm trying to I'm trying to work through there are some pieces that you're asking for that are not trivial to make.

We don't have a full reasoning system in place. We need to make one, and so we need to make some code without the full reasoning system. And I'm trying to figure out how to do semi reasoning to get little bits of artifact. And one of the little bits of artifact is the pure git based file system without the execution ability. So without where it's just simply be acting like a database effectively.

Scott:

Okay.

Tom:

Where it's got Let's

Scott:

that's that's a really useful example. So let's take just git. Yep. The reasoning should know there is a thing called git, and here's what it does. And that's pure NL.

Tom:

Yeah. And I should I should be able to make a software module that is reasoned about first in text and then generated from code and then through a combination of me

Scott:

Oh, yeah. Yeah. Yeah. Yeah. Yeah.

Yeah. You should be able to do that. Yeah.

Tom:

Right. From a combination of the fact that I've already got code that sort of does it, plus I know what it needs to do, I should from that be able to coax the box to make the code out of the reasoning. Yes. Right? Which then allows me to plug it into this thing, which then gives us the backing store for the CRM.

Yep. And then we can start doing the CRM as just a, like, no parallel code execution, just single threaded, single agent. Yep. Yes. Yep.

Yep. Single agent LLMs with Git based file storage.

Scott:

Yep.

Tom:

Then that gives us reasoning. Yeah. And then from the reasoning

Scott:

I've got what? Sorry. Domains.

Tom:

Yeah. So that gives us a reasoning tool, which means we can create domains easily without cut, paste, clone, push, pull, all those things. And then we can use that to decide which domain to pursue more of. Does artifact need parallel execution capability? Does it need blockchain consensus?

Does it need to be hooked up to an NFT capable blockchain? Right. Right? Those kinds of things?

Scott:

Right. Right. Right. Exactly.

Tom:

This feels this feels strange. But

Scott:

I will bet you five good Scots pounds.

Tom:

I'm not taking any more bets. Sorry, buddy.

Scott:

Oh, you're worse.

Tom:

I am a wuss.

Scott:

Well, it's it's gonna stay out there. I I bet you buy good stocks pounds that if we do that, the CRM, we could do in half a day. That's not Possibly inside of a meeting.

Tom:

Because where does that where does that leave us, though? Like, what are we how does this little spanner relate to all of that?

Scott:

Right. Okay. So the spanner is actually separate from the CRM. So the CRM guys want something just to work. Okay.

We want Right.

Tom:

Well spanner.

Scott:

They don't need the spanner. They want a perfect system that will work for everyone.

Tom:

They want the spanner, but they want it implied. Everyone who receives software wants it to come with a spanner, but they need it implied. I think there were issues in the that the chat based interface of delivering software allows you to have a spanner that's very direct. Whereas

Scott:

Uh-huh.

Tom:

In in traditional software, you know, where would you even Sorry.

Scott:

Oh, shit. That sounded horrible. Sorry.

Tom:

Okay. I gotta go in, like, ten minutes anyway. Alright. Do you wanna just call it call it here?

Scott:

No. Let's use the ten minutes and and then anyway you want because I'm I'm

Tom:

Okay. I

Scott:

think I well, I am, content with where we're at with this, but you're about to see what do we do

Tom:

next. Yeah. I'm I'm always saying that though. One day, I'm gonna make a bot to just repeat that for me. But but I guess every the spanner the spanner is only really applicable because of this whole chat based interface explosion because a traditional app, you because it was all, like, you know, the app was always sprinkled all over the place, you couldn't really put a spanner on it.

Well, there's

Scott:

that is a fundamental, and maybe we should make a blog, on this. That is the fundamental problem with AI is you put it in one sentence and you get a thousand words out. And you go, what what are the implications of that? And it's like, yeah, maybe if you could you could compress that and then get rid

Tom:

of it. That wasn't what I was meaning. I was meaning on, like, a web two point o SaaS app. The application structure itself is is so I don't even know what the word is for it, but in chat, it's turn based. And if you've got a turn based app, then you can have a problem with how one of the turns went.

But a traditional conventional web app is not turn based. It's, like, all over the place with tabs and drop downs.

Scott:

That's a that's a

Tom:

really good analogy. Yeah. I like Right?

Scott:

Yeah. Yeah.

Tom:

And so even when we have even when we have the state board, like, here's an example of the state board popping up. Even when we have the state board, you still have this turn based notion where you can have a problem with the exact state of this complex widget right now. Right? And that's that's what I'm getting at, that the spanner hasn't really been it hasn't fitted into the flow. The spanner requirement was always there from every customer since eternity.

They wanna be able to say, I was doing this, and it didn't work out one, and it sucks, and you better damn well fix it. Pretty pleased.

Scott:

That is that is that okay. So I know you've only got eight minutes left, but that is worth using those eight minutes on what you've just said. So people's interaction with software is software presents itself, I do this, and it's like, you know, the person sends itself saying, I want to do that. They might not actually agree. Up to this point, where do you go?

Tom:

What do you do? Right? Yeah. And it's complicated. You either switch vendors or something like that.

But there's there's something about chat interfaces that first of all enables the spanner, but also makes the fix be pluggable, which means that a universal app is sort of upon us. And if we can get the spanner, if you could have the app become anything you wanted just by how you use the spanner and how the spanner replied

Scott:

I know.

Tom:

Then your app can be anything.

Scott:

Hyper personalized hyper personalized apps, which is what we were talking about

Tom:

with the That's what we're trying to do. That's what we're trying to do. Right? Yeah. And so it sort of feels like, yes, we need this interface.

Okay. I'm getting I'm getting good results out of this interface. It's easy to maintain. It's it's easy to keep up. You know, new features are falling out the sky because other devs are working on it.

Scott:

Can I, is this suitable for, me to start working on this other than cursor or not yet?

Tom:

You're welcome to use it. I always it's always reasonably stable, m c p dot dreamcached dot dot land. But Yep. The question what we need is what's the what are the tools that are required so that what happens when I click this button? Number one.

Yeah. Number two, how do we attach context to each chat? I think I've got that figured out. So if I can just have a little bit of time to work on that one. And then number three is what do you want it to do?

Like, you you need to start being able to actually tell me what you want it to do to do reasoning, and you need to do some reasoning work on it. Like, what's the first feature you want if you had to pick one? What is

Scott:

Let let me give you that one. So number two is your partnership. Right? Number one, clicking on the

Tom:

Pero.

Scott:

Two is direct stock, which implies

Tom:

Yeah. But where does it go? Like, I know what it's meant to do, but specifically, like

Scott:

Yeah. It it well, we talked about that before. I'll not go through that. Okay? And then there's implied stocks, which are already defined

Tom:

in terms of the core definitions of dream catcher. So how do we get the reasoning that's happening right now in the reasoning directory? How do we get it inside of this app so that you can have a chat about the dream catcher right here.

Scott:

Well, why would you oh, why would you want to talk about the dream catcher? You don't really want to talk about the dream catcher if the dream catcher is doing something that you cognitively thought it should do and didn't do,

Tom:

and you're like No. I mean, we wanna use this tool right now to do reasoning about the dream catcher. Right?

Scott:

Okay. So when you hit the spanner, you can talk about either thread of the context of what you're talking about. So if we're talking about the CRM, Click that. You and I may want to talk about the the core definitions of the Dreamcatcher saying, woah, we just had three guys saying something. Tell us what the implications are to the definitions, then we want the definitions cycling around and the eval is just sitting up there.

That was something we really need to get onto and update, push, and then everything improves. Now with o one, it's given a few good men, let's say, a few good people. Having something pushed to you saying, oh, there's these guys trying to do this in the CRM, and you've got a PR guy, and you've got a marketing guy, and you've got this other blah blah blah whatever insurance guy. They've got all the problem with how the gene catcher works, then when we click that, we should be talking to the gene catcher, but it's the same pattern. We're talking about the core.

So we got domains. The DreamCatcher is domain. The CRM is a domain. The, insurance guy is a domain. We need to have context and then we're talking about that.

But then the really important thing is what is the implications of if we change any of these definitions? Because that very quickly gets beyond the can of any human mind.

Tom:

Yes. So how do how how do I implement that? Like, that's No. That's all great.

Scott:

What do I do? All you all you need to do all we need to do is when I hit that,

Tom:

there are You hit the you hit the spanner. Which

Scott:

yeah. You hit the spanner. Which, definitions are you talking to now? Then I can do the prompting, around, talking to that and then doing the

Tom:

same thing. Yeah. But, you know, this is the problem, though. Like, now we end up now we need naps because now we've got, like, an agent that is prompted up with a certain set of definitions and some behavior that we've tested.

Scott:

Which is the implications.

Tom:

Yeah. Which is

Scott:

why evals is important. It's like Yeah.

Tom:

But now we're now now now we're better than we were before. We just need all the stuff.

Scott:

No. No. No. No. No.

No. No. No. I think we're we're closer than we think.

Tom:

Alright. Well, anyway, I wanna stop the recording and upload it because I gotta go. Yeah. No worries, man. Stick around.

I can stick around for

Scott:

Yeah. Can you