2024-12-10 o1 pro concat tool to include folders in prompts - Summary and Next Steps.md 

Throughout the conversation, Tom and Scott discuss a new workflow enabled by a large-context LLM (“01 Pro”) and a custom “concat” tool. Instead of coding by hand, Tom now focuses on reasoning, specifying interfaces, and providing entire codebases as context to the LLM. At 0:52–1:16, Tom acknowledges shifting to a “reason first” approach and using the model to generate code. By 1:43–2:07, Tom realizes passing the entire codebase into context is now affordable and effective. At 4:51–9:10, Tom explains how “concat” creates a single dump file of multiple code files, including token counts, allowing the model to see all relevant content at once.

From 10:05–11:53 and 17:05–21:02, they note the model’s improved logical consistency, fewer contradictions, stable code outputs, and refined error detection—significantly better than earlier models. At 24:00–29:59, Tom describes reversing specs from code and using “concat” to selectively provide readme files for coherent summaries. By 31:08–33:00, they confirm that a 128k-token context is ample. The conversation (41:31) also touches on advanced model capabilities (like Sora for images/videos) that infer physics and reality.

Finally, at 45:50–55:12, they agree that Scott will install “concat” and use it to reason about domain definitions. They’ll integrate results later and eventually address meta reasoning once foundational reasoning is established.

Next Steps:
• Scott will install and use “concat” to process and summarize domain files (45:50–46:40).
• Scott will manually define domain concepts and interfaces (49:54–53:01).
• Tom will integrate these domain definitions and specifications into a higher-level reasoning layer (54:43–55:12).
• Both will eventually revisit meta reasoning after completing core domain reasoning.